{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from notebook import *\n",
    "hist_size=10000000\n",
    "# if get something about NUMEXPR_MAX_THREADS being set incorrectly, don't worry.  It's not a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": false,
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"namebox\">    \n",
    "Double Click to edit and enter your\n",
    "\n",
    "1.  Name\n",
    "2.  Student ID\n",
    "3.  @ucsd.edu email address\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div style=\" font-size: 300% !important;\n",
    "    margin-top: 1.5em;\n",
    "    margin-bottom: 10px;\n",
    "    font-weight: bold;\n",
    "    line-height: 1.0;\n",
    "    text-align:center;\">Lab 5: Parallelism</div>\n",
    "<div style=\" font-size: 100% !important;\n",
    "    line-height: 1.0;\n",
    "    text-align:center;\">Or, if we are being honest, mostly it's Caches Part III</div>\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Modern computers exploit parallelism in many ways:  \n",
    "\n",
    "1.  They can execute multiple threads at once.\n",
    "2.  They can execute instructions in parallel.\n",
    "3.  They can handle multiple memory requests at once.\n",
    "\n",
    "We are going to look at each of these kinds of parallelism, but we'll spend the most time on threading, since it's the form of parallelism that's most apparent to the programmers and the one that takes the most effort to exploit.  Not surprisingly, we will find that much of what makes parallel code fast and slow has to do with how it uses memory.\n",
    "\n",
    "In particular we are going to study:\n",
    "\n",
    "1. Instruction level parallelism.\n",
    "3. Memory-level parallelism.\n",
    "4. Thread-level parallelism.\n",
    "    1.  How it works.\n",
    "    2.  How to use it.\n",
    "    4.  Cache Coherence\n",
    "    5.  Synchronization\n",
    "    4.  False sharing\n",
    "5.  Hyperthreading\n",
    "6.  The OpenMP extensions for C/C++\n",
    "\n",
    "This lab includes a programming assignment.\n",
    "\n",
    "Check the course schedule for due date(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# FAQ and Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "* There are no updates, yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Additional Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "If you want to learn _a lot_ more about optimizing matrix multiply, try this paper:  https://www.cs.utexas.edu/~flame/pubs/GotoTOMS_revision.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Before You Do Anything Else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Run this cell.  It'll fix your git repo history so you can successfully merge in updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./fix-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Browser Compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We are still working out some bugs in some browsers.  Here's the current status:\n",
    "\n",
    "1.  Chrome -- well tested.  Preferred option. **Required for Moneta**\n",
    "2.  Firefox -- seems ok, but not thoroughly tested.\n",
    "3.  Edge -- seems ok, but not thoroughly tested.\n",
    "4.  Safari -- not supported at the moment.\n",
    "5.  Internet Explorer -- not supported at the moment.\n",
    "\n",
    "At the moment, the authentication step must be done in Chrome.  You usually _will not_ have to re-authenticate between labs, so if things work OK for the first, things will probably work here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# About Labs In This Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "_This section is the same in all the labs.  It's repeated here for your reference._\n",
    "\n",
    "Labs are a way to **learn by doing**.  This means you _must_ **do**.  I have built these labs as Jupyter notebooks so that the \"doing\" is as easy and seamless as possible.\n",
    "\n",
    "In this lab, what you'll do is answer questions about how a program will run and then compare what really happened to your predictions.  Engaging with this process is how you'll learn.  The questions that the lab asks are there for several purposes:\n",
    "\n",
    "1.  To draw your attention to specific aspects of an experiment or of some results.\n",
    "2.  To push you to engage with the material more deeply by thinking about it.\n",
    "3.  To make you commit to a prediction so you can wonder why your prediction was wrong or be proud that you got it right.\n",
    "4.  To provide some practice with skills/concepts you're learning in this course.\n",
    "5.  To test your knowledge about what you've learned.\n",
    "\n",
    "The questions are graded in one of three ways:\n",
    "\n",
    "1. \"Correctness\" questions require you to answer the question and get the correct answer to get full credit.\n",
    "2. \"Completeness\" questions require you to answer the question.\n",
    "3. \"Optional\" questions are...optional.  They are there if you want to go further with the material.\n",
    "\n",
    "Some of the \"Completeness\" problems include a solution that will be hidden until you click \"Show Solution\".  To get the most from them, try them on your own first.\n",
    "\n",
    "Many of the \"Completeness\" questions ask you to make predictions about the outcome of an experiment and write down those predictions.  To maximize your learning, think carefully about your prediction and commit to it.  **You will never be penalized for making an incorrect prediction.**\n",
    "\n",
    "You are free to discuss \"Completeness\" and \"Optional\" questions with your classmates.  You must complete \"Correctness\" questions on your own.\n",
    "\n",
    "If you have questions about any kind of question, please ask during office hours or during class."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## How To Succeed On the Labs\n",
    "\n",
    "Here are some simple tips that will help you do well on this lab:\n",
    "\n",
    "1.  Read/skim through the entire lab _before_ class.  If something confuses you, you can ask about it.\n",
    "2.  Start early.  Getting answers on edstem/piazza can take time.  So think through the lab questions (and your questions about them) carefully.\n",
    "    1. Go through the lab once (several days before the deadline), do the parts that are easy/make sense\n",
    "    2. Ask questions/think about the rest\n",
    "    3. Come back and do the rest.\n",
    "3.  Start early.  The DSMLP cluster gets busy and slow near deadlines.  \"The cluster was slow the night of the deadline\" is not an excuse for not getting the lab done and it is not justification for asking for an extension.\n",
    "4.  Follow the guidelines below for asking answerable questions on edstem/piazza.\n",
    "\n",
    "You may think to yourself: \"If I start early enough to account for all that, I'd have to start right after the lab was assigned!\"  Good thought!\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**The Cluster Will Get Slow** DSMLP and our cloud machines will get crowded and slow _before every deadline_.  This is completely predictable.  DSMLP can also get crowded due to deadlines in other courses.  You need to start early so you can avoid/work around these slowdowns.  Unless there's some kind of complete outage, we will not grant extensions because the servers are crowded.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Getting Help\n",
    "\n",
    "You might run into trouble while doing this lab.  Here's how to get help:\n",
    "\n",
    "1.  Re-read the instructions and make sure you've followed them.\n",
    "2.  Try saving and reloading the notebook.\n",
    "3.  If it says you are not authenticated, go to the [the login section of the lab](#Logging-In-To-the-Course-Tools) and (re)authenticate.\n",
    "4.  If you get a `FileNotFoundError` make sure you've run all the code cells above your current point in the lab.\n",
    "4.  If you get an exception or stack dump, check that you didn't accidentally modify the contents of one of the python cells.\n",
    "5.  If all else fails, post a question to edstem/piazza.\n",
    "\n",
    "## Posting Answerable Questions on Edstem/Piazza\n",
    "\n",
    "If you want useful answers on edstem/piazza, you need to provide information that is specific enough for us to provide a useful answer.  Here's what we need:\n",
    "\n",
    "1.  Which part of which lab are you working on (use the section numbers)?\n",
    "2.  Which problem (copy and paste the _text_ of the question along with the number).\n",
    "\n",
    "If it's question about instructions:\n",
    "\n",
    "1.  Try to be as specific as you can about what is confusing or what you don't understand (e.g., \"I'm not sure if I should do _X_ or _Y_.\")\n",
    "\n",
    "If it's a question about an error while running code, then we need:\n",
    "\n",
    "1.  If you've committed anything, your github repo url.\n",
    "2.  If you've submitted a job with `cse142` you _must_ provide the job id.  It looks like this: `544e0cf2-4771-43c3-86f8-1c30d7af601f`.  With the id, we can figure out just about anything about your job.  Without it, we know nothing.\n",
    "3.  The _entire_ output you received.  There's no limit on how long an edstem/piazza post can be.  Give us all the information, not just the last few lines.  We like to scroll!\n",
    "\n",
    "For all of the above **paste the text** into the edstem/piazza question.  Please **do not provide screen captures**.  The course staff refuses to type in job ids found in screen shots.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**We Can't Answer Unanswerable Questions** If you don't follow these guidelines (especially about the github repo and the job id), we will probably not be able to answer your question on edstem/piazza.  We will archive it and ask you to re-post your question with the information we need.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Keeping Your Lab Up-to-Date\n",
    "\n",
    "Occasionally, there will be changes made to the base repository after the\n",
    "assignment is released.  This may include bug fixes and updates to this document.  We'll post on piazza/edstem when an update is available.\n",
    "\n",
    "In those cases, you can use the following commands to pull the changes from upstream and merge them into your code.  You'll need to do this at a shell.  It won't work properly in the notebook.  Save your notebook in the browser first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "First, you need to run the `./fix-repo` in the cell at the top of this section.  After that you can run  \n",
    "\n",
    "```\n",
    "./pull-updates\n",
    "```\n",
    "\n",
    "at a terminal.\n",
    "\n",
    "Then, reload this page in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Editing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "For programming assignments, it can be nice to use an editor other than the jupyter notebook editor.  Below are some student-provided instructions for using various editors with datahub.  These are supported, but if you provide feedback about them, we'll try to update and improve them.\n",
    "\n",
    "### VSCode\n",
    "\n",
    "1. Install the “SSH-Remote” extension in VS Code\n",
    "2. Open the Command Palette in VS Code\n",
    "3. Type “remote-ssh” and click “Remote-SSH: Connect current window to host”\n",
    "4. It should ask you to put in the command: ssh {your_username}@dsmlp-login.ucsd.edu\n",
    "5. If it asks for fingerprint authorization or something similar, accept\n",
    "6. It should then ask for your password (your ucsd.edu email password)\n",
    "7. Once logged in, click \"Open Folder\" in the file explorer sidebar\n",
    "8. Open the folder pertaining to the lab you are working on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## How To Use This Document\n",
    "\n",
    "You will use Jupyter Notebook to complete this lab.  You should be able to do much of this lab without leaving Jupyter Notebook.  The main exception will be some of the programming assignments.  The instructions will make it clear when you should use the terminal.\n",
    "\n",
    "### Logging In\n",
    "\n",
    "If you haven't already, you can go to [the login section of the lab](#Logging-In-To-the-Course-Tools) and follow the instructions to login into the course infrastructure.\n",
    "\n",
    "### Running Code\n",
    "\n",
    "Jupyter Notebooks are made up of \"cells\".  Some have Markdown-formatted text in them (like this one).  Some have Python code (like the one below).\n",
    "\n",
    "For code cells, you press `shift-return` to execute the code.  Try it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"I'm in python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Code cells can also execute shell commands using the `!` operator.  Try it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!echo \"I'm in a shell\""
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Telling What The Notebook is Doing\n",
    "\n",
    "The notebook will only run one cell at a time, so if you press `shift-return` several times, the cells will wait for one another.  You can tell that a cell is waiting if it there's a `*` in the `[]` to the left the cell:\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You'll can also tell _where_ the notebook is executing by looking at the table of contents on the left.  The section with the currently-executing cell will be red:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAADcAAAA1CAYAAADlE3NNAAABRGlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rAycDPIMYgwyCSmFxc4BgQ4ANUwgCjUcG3awyMIPqyLsisHbud/s7f09wx77xWn4rBr1hM9SiAKyW1OBlI/wHitOSCohIGBsYUIFu5vKQAxO4AskWKgI4CsueA2OkQ9gYQOwnCPgJWExLkDGTfALIFkjMSgWYwvgCydZKQxNOR2FB7QYDHxdXHRyHAxNjQPJCAc0kHJakVJSDaOb+gsigzPaNEwREYSqkKnnnJejoKRgZGhgwMoDCHqP75FhyWjHWbEWKJwQwMhq1AQSGEWLYoA8Oe3wwMQrsRYlp5DAyCDQwM++MLEosS4Q5g/MZSnGZsBGGLhzEwcHb9//8C6FHuf0C79P7//8H7///vegYG9iIGhm4rACicXg0u2h90AAAAOGVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAACoAIABAAAAAEAAAA3oAMABAAAAAEAAAA1AAAAABCNvQ0AAAJGSURBVGgF7VkxayJBFP52XVC4YHNRxFoQRLSwvLTaWopgArkfYK2IlUViY5PzL1jZqI2dlbbaiWisREXUQnJYyF6S2WLvAgvZ3cwes8NMNTvz9r3ve9+beSwrnc/nV3A6ZE55abQEObeqK5QTyjGYAWW/3zMIiw4k6fV90HHFnhdxobCniTlEQjlzeWLPShmNRuyhooWI3JZWxnA4tGJOxdZuTHHmaFXJ//bDtXKKE9ncbDbYbremXMdiMXi9XlO2Vox2ux0cIVcqlTAej01hqVQqyOVypmw/M7pcLmg2m+h0OjgcDs6QU1X1Mxz6vqr+0edfmby8/Mbd3S0Wi4VWCfF43BlyXwFp993HxweNWCqVQr1eRzAYhJzJZJBIJEDKw0rG7YJw4r31eo1ut4tAIICnp18aMRJHbjQaqNVq6PV6GAwGTsR23OdkMtFipNNpXF190+PJpDaz2SwikQhWq5W+4aYJUY6McDj8Abbe53w+n2vLMhqNaqRms5kxuQ+rLntIJpMg4vT7fSyen3X0unL6igsnfr8fxWIRpM/9vL/X+hxpDVyQI3oUCgXk83mcTidUq1Xc3Pz42+darRY1zUKhEJbLpSl/19ffTdmZMSqXyyCtrd1uYzqdAlY/uOx+W1mN86+93ZjclKWRsoKcUVbcsCaUc4NKRhglchMZbfCwJn6EuFVFcaEI5RjMgDKfzxmERQeSdDweuW0Fiizze6coHo+HTg0w6IVv5bguS67JSZLE4GmhA4nrM8dvH3gXX+G5LN8AO2tDVwI9BI4AAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### What to Do Jupyter Notebook It Gets Stuck\n",
    "\n",
    "First, check if it's actually stuck: Some of the cells take a while, but they will usually provide some visual sign of progress.  If _nothing_ is happening for more than 10 seconds, it's probably stuck.\n",
    "\n",
    "To get it unstuck, you stop execution of the current cell with the \"interrupt button\":\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC4AAAAiCAYAAAAge+tMAAABRGlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rAxcDBIMjAw6CZmFxc4BgQ4ANUwgCjUcG3awyMIPqyLsisycd66uYamIqqf/v/Otp99zlM9SiAKyW1OBlI/wHitOSCohIGBsYUIFu5vKQAxO4AskWKgI4CsueA2OkQ9gYQOwnCPgJWExLkDGTfALIFkjMSgWYwvgCydZKQxNOR2FB7QYDHxdXHRyHAxNjQPJCAc0kHJakVJSDaOb+gsigzPaNEwREYSqkKnnnJejoKRgZGhgwMoDCHqP75FhyWjHWbEWKJwQwMhq1AQSGEWLYoA8Oe3wwMQrsRYlp5DAyCDQwM++MLEosS4Q5g/MZSnGZsBGGLhzEwcHb9//8C6FHuf0C79P7//8H7///vegYG9iIGhm4rAKtIXcvOgP2JAAAAOGVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAACoAIABAAAAAEAAAAuoAMABAAAAAEAAAAiAAAAALSq6z8AAAJiSURBVFgJ7Zi/ayJBFMe/HoLCiZ0WdrERRFDwHxBBazstPCHXCBexUHJgIZYXW3P+BzaCIFpZWSgKFwQtDCqaRsHCH4WQEyXqXd6AksQznM6uENgBnTdvmPc++93Zt8PK/jw3fMD26QMyM2QJ/Nx3Tl6tVs+d82A+hUKB5XJ5cP7VBD2ch1qlUjk0JYr/mHzSHn91G88wkBTfijwajdDv97dDQfvFYoH1es1iyoWI/PT0hGQyiVwuh+l0ykKqVCo4HA5cX3/H3d0vjMdjuN1urnT1eh2pVAqJRALc4I+Pv+HzfUGv1wOVM5PJBKVSiXa7jWw2i1KphNlsBpvNxg1OV10ulxEMBvnBb25+MGir1Yp4PA6tVstUpQu6uvoGUknoRvBcig+HQ+TzeWg0Gtze/oRK9XnHeH/fRLPZ3I2FNrjAG40G46G9/BKanHQxgUBgx2uxWHa2EAYXOClOTafT7bHo9XrQT6zGVccNBgPj6nQ6YvEdjMsFbjabWQUpFAroPTzsJZlMJqjVant+IRxc4Gq1mpUmquNfLy9ZHadqMp/PUSwW4fF44Pf7sX0WhACmGDKZjK+qUBCv14vBYIB0Oo1oNIpYLEZubDYb1rtcLhiNRmYL8UfQoVCIH5xgIpEInE4nMpkMWq0WVqsVexHZ7XbmFwKYYtALLhwOP7/wfMB7B+tjzsfvxfnfuWPyce1xoZQ8JY4EfopqPGskxXnUO2WtjJ7kUxaKseaYzxMyKlViQIgdU9rjYiv8Nr6k+FtFxB7Lu92u2Dn+Gf9lTSB7O97a29Ml9fQthfxkX1xcsHh/AeSdPWhpR3GTAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You can also restart the underlying python instance (i.e., the confusingly-named \"kernel\" which is not the same thing as the operating system kernel) with the restart button:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Once you do this, all the variables defined by earlier cells are gone, so you may get some errors.  You may need to re-run the cells in the current section to get things to work again.\n",
    "\n",
    "You can also try reloading the web page.  That will leave Python kernel intact, but it can help with some problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Common Errors and Non-Errors\n",
    "\n",
    "1.  If you get `sh: 0: getcwd() failed: no such file or directory`, restart the kernel.\n",
    "2.  If you get `INFO:MainThread:numexpr.utils:Note: NumExpr detected 40 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.`.  It's not a real error.  Ignore it. \n",
    "3.  If you get a prompt asking `Do you want to cancel them and run this job?` but you can't reply because you can't type into an output cell in Jupyter notebook, replace `cse142 job run` with `cse142 job run --force`. (see useful tip below.)\n",
    "4.  If you get an `Error: Your request failed on the server: 500 Server Error: Internal Server Error for url=http://cse142l-dev.wl.r.appspot.com/file`, trying running the job again.\n",
    "5.  Sometimes `cse142 job run` will just sit there and seemingly do nothing.  Weirdly, interrupting the kernel (button above) seems to jolt it awake and cause it to continue.\n",
    "6.  If you get an error like this, stop your datahub server and restart it.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "7. If you get `http.cookiejar.LoadError: '/home/youruserrname/.djr-cookies.txt` does not look like like a Netscape format cookies file.` remove the file and re-authenticate.\n",
    "8.  The table of contents disappears and/or the questions are not highlighted like they usually are.  Do this:    \n",
    "    1.  Go to the file browser in jupyter\n",
    "    2.  At the top, there is tab labeled \"Nbextensions\".  Click on it.\n",
    "    3.  find \"Table of Contents (2)\".  It should be checked.  Un check it, and check it again.\n",
    "    4.  Click the \"refresh button\" (circular arrows)at upper right.\n",
    "    5. Reload your notebook.\n",
    "9.  You produce too much output from a program and your notebook refuses to open because it's too big.  Try\n",
    "    1.  Backup up your notebook!\n",
    "    2.  This will work, but it will clear _all_ your output: https://stackoverflow.com/a/47774393/3949036\n",
    "    3.  You can open the notebook file in a text editor and remove the output manually.\n",
    "    "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAADkAAAApCAYAAACPzoEeAAABRGlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rAySDFwMOgyGCemFxc4BgQ4ANUwgCjUcG3awyMIPqyLsgsNhlLvoOd7mlp6cLz5mlnTsVUjwK4UlKLk4H0HyBOSy4oKmFgYEwBspXLSwpA7A4gW6QI6Cggew6InQ5hbwCxkyDsI2A1IUHOQPYNIFsgOSMRaAbjCyBbJwlJPB2JDbUXBHhcXH18FAJMjA3NAwk4l3RQklpRAqKd8wsqizLTM0oUHIGhlKrgmZesp6NgZGBkyMAACnOI6p9vwWHJWLcZIZYYzMBg2AoUFEKIZYsyMOz5zcAgtBshppXHwCDYwMCwP74gsSgR7gDGbyzFacZGELZ4GAMDZ9f//y+AHuX+B7RL7///H7z///+uZ2BgL2Jg6LYCAM3XWvMCZhWeAAAAOGVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAACoAIABAAAAAEAAAA5oAMABAAAAAEAAAApAAAAAPS5PxgAAALOSURBVGgF7Vm9j2lBFD/uXh95u+sjm6DZrChUClohehXJJhvxJ0hESUcnGiVbicoWQiNEoZFQqHw0NArFKhDsCpENnpmE7MaVd3nD+jqJ3LnnzPzOOXPOnJlxOePxeA5nTtSZ+4fduwgn6U6nc/bB5MwXdO5eXkS6Xp08lzS+iEjShUKBVcC4XC48PT2BVCpd6//5+Qnv7+/Q7XbXZEfBQNWVDX18fMyLxSJj11qtNu/1eowyUsx8Pr8zFOt0vb+/h6+vL8bAoAiKxWJG2TEwWTt5DMbuasOvODkYDGA0Gu1q89bjDupks9kEm80GBoMBdDod2O126Pf7WxvNZgCaxHq9DpPJBGg2A0j0mU6n4HA4AFVip9MJw+EQ3t7ewOVyQSgUIqFihREIBCASicBsNgOBQAB0PB7HQo1GA0qlctWRdKNarUKj0QCfzwcmkwnDo20pGAwCSl+RSEREZSwWg3A4DBaLBYxGIyQSCaArlQoGf319hUwmQ0QRE0ir1cJslKZLslqtOF1vbsglVDqdBpVKBV6vF6vR6/VAKRQKcLvdOI2WyvfxXGxya7Bo20G67+5u12S7MtrtNqjV6tVwHo8HdDQaBbPZvGLuo4GKQCqVwtB+vx+Q4u8kkUjweuVwON/ZW7ez2Sw+dZVKJfB4PKvx9OKksnrZV6NcLkMul8PwyWSSUc3LywvI5XJGGVsmWouosKEfWv9LolAhEAqFy/dfezKlMyljKFR9RqMxKbyjxKHQwr+9/XOUxpEyin5+fsZYWq2WFOYajkwmAz6fj08fa8IFAy0XEvvk4+MjLLfEH3q2ub9suu5s4m+D/a++/6PjoGfXH7N7wJerkwec7L2qukZyr9N7QHDWkURHJXQ1YqKHh4e9XX6Z9G3L46DSzGbQKf8lef3gwybCp9CH9Zo8BWc22Xh1ctPMnBqfPtqPNARnkqao88/Yi3DyL0l+mHXQ5TN9AAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Useful Tips\n",
    "\n",
    "1.  If you need to edit a cell, but you can't you can unlock it by pressing this button in the tool bar (although you probably shouldn't do this because it might make the lab work incorrectly.  A better choice is to copy and paste the cell, _and then_ unlock the copy):\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAACEAAAAkCAYAAAAHKVPcAAABRGlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rAycDPIMYgwyCSmFxc4BgQ4ANUwgCjUcG3awyMIPqyLsisHbud/s7f09wx77xWn4rBr1hM9SiAKyW1OBlI/wHitOSCohIGBsYUIFu5vKQAxO4AskWKgI4CsueA2OkQ9gYQOwnCPgJWExLkDGTfALIFkjMSgWYwvgCydZKQxNOR2FB7QYDHxdXHRyHAxNjQPJCAc0kHJakVJSDaOb+gsigzPaNEwREYSqkKnnnJejoKRgZGhgwMoDCHqP75FhyWjHWbEWKJwQwMhq1AQSGEWLYoA8Oe3wwMQrsRYlp5DAyCDQwM++MLEosS4Q5g/MZSnGZsBGGLhzEwcHb9//8C6FHuf0C79P7//8H7///vegYG9iIGhm4rACicXg0u2h90AAAAOGVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAACoAIABAAAAAEAAAAhoAMABAAAAAEAAAAkAAAAAP1oLUoAAAHGSURBVFgJ7VZdq4JAEB0/+jIIqocg9LH//4sCjYosKIvMitLLEfbSLce7q0Y9eEAWd3dmzp7ZnV1ts9kk9GHoH46fhq9JiCzUStRKCAVE+xV7whRsuHa/39Nut6Pz+UxJolbXDMMgy7JoNBpRo9HgQlAuCd/3abVascYyA1EUERYymUyo2WxmmrDpuFwuBBJV4Ha70Xw+Z12xShyPxz/yDwYD0nWWc2aA0+lE+IAwDDPnoJMlAfYCpmmS4zjiV7rFXprNZun8OI5ZO7WlsW7KDZQigdW5rkvb7bYUCzYdMl4Xi0W68w+HA7Xb7fQ4ytg9zymsRBAEvwqgfnieR3l5fw78+F+IxPV6fTly6IMyRVCIBHb8/X5/iYe9AYVUoUxivV7nnnkUJaiiAiUScP5fGYdCy+VShQNfrDgvnU6HHgtZ1rxWq5XVzfYpHVFcQLiIqoZSOqoOLvxJkVB9Rwjnsi2bjse7H5ttOp0q36J4CAnggcOBJdHtdtOgogriai+DXq/HmrPpgBK2bZOmaayx7ABOy3g8ZqezSsCi3++nFxPeBXhpqQIpwJEeDoe5qcwlgaBwgu+dYNPxzqDPvmsSQpFaCaHEDz84qm5DUiF+AAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### The Embedded Code\n",
    "\n",
    "The code embedded in the lab falls into two categories:\n",
    "\n",
    "1.  Code you need to edit and understand.\n",
    "2.  Code that you do not need to edit or understand -- it's just there to display something for you.\n",
    "\n",
    "For code in the first category, the lab will make it clear that you need to study, modify, and/or run the code.  If we don't explicitly ask you to do something, you don't need to.\n",
    "\n",
    "Most of the code in the second category is for drawing graphs.  You can just run it with shift-return to the see the results.  If you are curious, it's mostly written with `Pandas` and `matplotlib`. The code is all in `notebook.py`.   These cells should be un-editable.  However, if you want to experiment with them, you can copy _the contents_ of the cell into a new cell and do whatever you want (If you copy the cell, the copy will also be uneditable).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Most Cells are Immutable** Many of the cells of this notebook are uneditable. The only ones you should edit are some of the code cells and the text cells with questions in them.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Pro Tip** The \"carrot\" icon in the lower right (shown below) will open a scratch pad area.  It can be a useful place to do math (or whatever else you want.\n",
    "    \n",
    "![image.png](attachment:image.png)\n",
    "</div>"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEIAAAA7CAYAAADPeVzhAAABRGlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rAycDPIMYgwyCSmFxc4BgQ4ANUwgCjUcG3awyMIPqyLsisHbud/s7f09wx77xWn4rBr1hM9SiAKyW1OBlI/wHitOSCohIGBsYUIFu5vKQAxO4AskWKgI4CsueA2OkQ9gYQOwnCPgJWExLkDGTfALIFkjMSgWYwvgCydZKQxNOR2FB7QYDHxdXHRyHAxNjQPJCAc0kHJakVJSDaOb+gsigzPaNEwREYSqkKnnnJejoKRgZGhgwMoDCHqP75FhyWjHWbEWKJwQwMhq1AQSGEWLYoA8Oe3wwMQrsRYlp5DAyCDQwM++MLEosS4Q5g/MZSnGZsBGGLhzEwcHb9//8C6FHuf0C79P7//8H7///vegYG9iIGhm4rACicXg0u2h90AAAAOGVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAACoAIABAAAAAEAAABCoAMABAAAAAEAAAA7AAAAAD+nIZMAAAQQSURBVGgF7Vu5SzQxFP/N7niAeIAHKHg1uzYWFoJooVhaCCKIIAiKjQdiIzaihRZ2wgqCgvgP+AfY2Hp0goKKICgqqIh4IJ74+fLt7Do7O0swM1lnN4HZmWReXt775eUlk7fRvr4TfpnOz89RVFT0y9rA09MT+vr6MDIygubmZguf6elpFBYWYnR0FN3d3RgcHERxcTGofGVlBXl5eazO1dUVrq+vUVtba+HBW+DjJfwrdIFAEDk5OQiFQri/v8fl5SWmpqawubkpJKIuVFuwsqb97wdN07g4EZ2u+zE5OYm5uTkMDAywesFgEL29vVw87Ii0ZA4NO6F4yx8eHpCVlcUu3jp2dEm1CDuheMsNH8FLn4jOcz4ikTIi7xQQYfQUEAoI80BSFqEsQlmEGQFlEWY8hHyE3+/H29ubmaNHc0JL7Lu7O7y8vODz89Oj6kfFFgIiysb7T0JDw/vqRzVQQISxUEAoIKLDgp6E9iNon/D09BTv7+9mri7kMjIyUFlZiZKSEgt3J+QQAoJAqKmpQW5urkU4pwseHx9xeHgYFwgn5BDyEWQJMkAgUKkdO8tzQg4hIJzu9WTyU0CE0RfyETw9SEGgm5ubuKTZ2dmgrXifT35/nJ2d4fb2FtXV1SgoKBCbNeJqF1PY1dXFGtvb22Nv8vPzEQgEQIJ8fHygtLQUS0tL0nwNCUE+paOjg90bGhqwvLwMKV0xOzsbgae+vp6F69ra2lhQpqmpCf39/aBZQVaij0TD8T4/P7NmpQCRSMHh4WG0tLRgaGgoEZnr71z3EaQB7VsY6ejoCPPz89jZ2QFZBSUCY319ncUxy8rKDFJH79vb29ja2mI8aUgaiQLIJI8UICoqKlgEm0J0RiIQ2tvbjSwL272+vkbyTj8sLi5id3fXwpaAWF1dlQMEtd7Z2WkRQmYBLdETJSkWQdt5ExMT+GkRhlA0hdL/HdxO5LAPDg5YM+Qox8fH2XNVVRXGxsbkWMTFxQWOj48xMzNj0XdhYQH7+/uWcqcLaJqmixJtLxqJAsmtra1ygKBGqefr6uqM9iN3Wsz8hZT06TMZIGRmZrK/IFHb5eXlTATXfQRNhz09PayxxsZGdo/9oamNvi5F/o8VyzNRnpb0GxsbJhLXgVhbWzM1+FczaTk04nWGAiKMigJCAWEeIEIWQctWWZ/P1I7dMtkJOYRin05so5v7xT5Hyrq5nS8EhL3Y3nsjNDS8p669xAqIMDYKCAWEeZjoJycnoLMrsedXYvPmaqmX074Pf7CTCnaK25WnGhQ6HQahi1dhXjqvAaUb4bZUVZC3Q3SKOfwE4eczL5NUoIsMDTtl0gWYyNAgIOIpzXvwzA5Ir5QzizBASBel43VOBIh0BoGA8aU7AIZ1/AMiNFczf7D9oQAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Showing Your Work\n",
    "\n",
    "Several questions ask you to show your work for calculations.  We don't need anything fancy.  Many of the questions ask you to compute something based on results of an experiment.  Your experimental results will be different than others', so your answer will be different as well.\n",
    "\n",
    "To make it possible to grade your work (and give you partial credit), we need to know where your answer came from.  This why you need to show your work.  For instance this would be fine as answer to \"On average, how many weeks do you have per lab?\":\n",
    "\n",
    "```\n",
    "Weeks in quarter/# of labs = 5/5 = 1 week/lab\n",
    "```\n",
    "\n",
    "2 significant figures is sufficient in all cases, but you can include more, if you want.\n",
    "\n",
    "If you are feeling fancy, you can use LaTex, but it's not at all required.\n",
    "\n",
    "When it's appropriate, you can also paste in images.  However, Jupyter Notebook is flaky about it.  Save your notebook by clicking the disk icon:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Answering Questions\n",
    "\n",
    "Throughout this document, you'll see some questions (like the one below).  You can double click on them to edit them and fill in your answer.  Try not to mess up the formatting (so it's easy for us to grade), but at least make sure your answer shows up clearly.  When you are done editing, you can `shift-return` to make it pretty again.\n",
    "\n",
    "A few tips, pointers, and caveats for answering questions:\n",
    "\n",
    "1. The answers are all in [github-flavored markdown](https://guides.github.com/features/mastering-markdown/) with some html sprinkled in.  Leave the html alone.\n",
    "2. Many answers require you to fill in a table, and many of the `|` characters will be missing.  You'll need to add them back.\n",
    "3. The HTML needs to start at the beginning of a line.  If there are spaces before a tag, it won't render properly.  If you accidentally add white space at the beginning of a line with an html tag on it, you'll need to fix it.\n",
    "4. Text answers also need to start at the beginning of a line, otherwise they will be rendered as code.\n",
    "5. Press `shift-return` or `option-return` to render the cell and make sure it looks good.\n",
    "6. There needs to be a blank line between html tags and markdown.  Otherwise, the markdown formatting will not appear correctly.\n",
    "\n",
    "\n",
    "You'll notice that there are three kinds of questions: \"Correctness\", \"Completeness\", and \"Optional\".  You need to provide an answer to the \"Completeness\" questions, but you won't be graded on its correctness.  You'll need to answer \"Correctness\" questions correctly to get credit.  The \"Optional\" questions are optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Logging In To the Course Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "    \n",
    "In the course you will use some specialized tools to let you perform detailed measurements of program behavior.  To use them you need to login with your `@ucsd.edu` email address using the instructions below. **You need to use the email address that appears on the course roster.  That's the email address we created an account for.  In almost all cases, this is your `@ucsd.edu` email address.**\n",
    "\n",
    "You'll do this periodically when you get an error about not being authenticated.  You can return to this notebook (or any other of the lab notebooks) to login at any time.\n",
    "\n",
    "Here's what to do:\n",
    "\n",
    "1.  Enter your `@ucsd.edu` email address in quotes after `login` below.  It'll take a few seconds to load.\n",
    "2.  Click the google \"G\" login button below and login with your `@ucsd.edu` email address. \n",
    "3. **Click the google button regardless of whether it says \"sign in\" or \"signed in\".  Then be sure to select your `@ucsd.edu account` if it shows you multiple google acocunts**\n",
    "4. You'll see a very long string numbers an letters appear above.  Click \"Copy it\" to copy it.\n",
    "\n",
    "**Note:** If it doesn't give you a choice about which account to log into and authentication fails, that means you are logged into a single Google account and that account is _not_ your `@ucsd.edu` account.  You'll have to log into your `@ucsd.edu` through Gmail or through Chrome's account manager and then try again.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Use Chrome** The login process doesn't seem to work properly with Safari or Firefox.  Use Chrome to login.  You can use any of the other compatible browsers you want for the doing the rest of the lab, and it should be fine.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "login(\"<Your @ucsd.edu email address>\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Next step:  Paste it below between the quote marks.  Press `shift-return`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cse142L.is_response": true,
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "token(\"your_token\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "It should have replied with\n",
    "\n",
    "``` \n",
    "You are authenticated as <your email>\n",
    "```\n",
    "\n",
    "You are now logged in!  Try submitting a job: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!cse142 job run \"echo Hello World\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "If you see \"Hello World\", you're all set.  Proceed with the lab!\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "Delete your token from the above cell. Because your token is esssentially your username and password combined, you should treat it like a password or ssh private key.  **Sharing your token with another student or posessing another student's token is an AI violation**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Your grade for this lab will be based on the following components\n",
    "\n",
    "| Part                       | value |\n",
    "|----------------------------|-------|\n",
    "| Jupyter Notebook           | 48%   |\n",
    "| Programming Assignment     | 50%   |\n",
    "| Post-lab survey.           | 2%    |\n",
    "\n",
    "No late work or extensions will be allowed.\n",
    "\n",
    "We will grade 5 of the \"completeness\" problems.  They are worth 3 points each.  We will grade all of the \"correctness\" questions.\n",
    "\n",
    "You'll follow the directions at the end of the lab to submit the lab write up and the programming assignment through gradescope. \n",
    "\n",
    "Please check gradescope for exact due dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Instruction Level Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "As you learned in CSE142, modern processors can exploit parallelism between instructions by fetching a large \"window\" of instructions and dynamically identifying instructions that can execute at the same time.  The algorithm it uses is pretty amazing and the fact that processors can do this at over 3GHz is mind-blowing.\n",
    "\n",
    "<div style=\"background:black; padding:0.5em; color:white; margin-left:auto; margin-right:auto; margin-top:1em; margin-bottom:1em;  text-align:center\"> [Meme Redacted For Space Reasons]</div>\n",
    "\n",
    "But how well does it work?  How much ILP can we get?  Where does ILP come from?\n",
    "\n",
    "## Let's Look at Our Machine\n",
    "\n",
    "Here's a cartoon of our processor's functional units.\n",
    "\n",
    "![image.png](img/issue_ports.png)\n",
    "\n",
    "The blue units on the left can all execute basic arithmetic operations (\"ALU\") and two of them can execute our old friend `lea`.  One interesting note about this diagram:  There are 2 load unit and one unit each for store addresses and store data.  This means that processor can issue 2 loads but only one store per cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Based on this image, what's the minimum achievable CPI for ALU operations on our CPU? For load operations? For all operations?\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "**Minimum possible ALU CPI**:\n",
    "    \n",
    "**Minimum possible load CPI**:\n",
    "\n",
    "**Minimum possible total CPI**:\n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\"> \n",
    "   \n",
    "It appears that the CPU can execute four ALU operations in one cycle.  So the minimum CPI would be 1/4 = 0.25.\n",
    "    \n",
    "For loads, it seems we can do 2 per cycle, so the CPI should be 1/2 = 0.5.\n",
    "\n",
    "All together (loads, stores, and ALU), it appears that 1/7 = 0.142 should be possible.\n",
    "    \n",
    "Note that one store instruction takes up both the \"store addr\" and \"store data\" unit, so only one store can execute per second on average.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Based on this design, do you think the designers are more concerned about achieving parallelism between memory operations or arithmetic (non-memory) operations?\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.question_type": "optional",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question optional\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "    \n",
    "Why does it \"execute\" the data and address for a store separately?\n",
    "    \n",
    "</div>\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Accounting For Branches\n",
    "\n",
    "In this section we are going look at instruction dependencies and critical paths quite closely, following what you learned in CSE142.  Branches and comparison operations complicate things a little bit.  For the most part we will ignore comparison instructions and branch instruction when calculating critical paths.  This is because the processor speculates on branches, this means that these instructions don't have much impact on the performance of the processor's out-of-order execution engine.  Since the processor only uses them to detect mis-speculations, it never has to wait on them.\n",
    "\n",
    "We do, however, count those instructions when calculating _how many_ instructions have executed.  While comparisons and branches don't contribute latency to the execution, they do eventually have to execute, so the processor's branch predictions can be checked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Experiments\n",
    "\n",
    "Here's a simple function (run the code to see it).  I've turned off optimizations and use `register` to get `i` into a register.\n",
    "\n",
    "Study the code and pay particular attention to the data dependences between the instructions in the inner loop body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"ilp.cpp\", function=[\"wide_1\", \"wide_2\", \"wide_3\", \"wide_4\", \"wide_5\"], name=\"ilp\", opt=\"-O0\", run=[], \n",
    "           cmdline=f\"--size 10000000\", \n",
    "           perf_cmdline=\"--stat-set PE.cfg --MHz 3500\")\n",
    "compare([t.source, t.cfg])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "    \n",
    "What's the critical path length through $n$ iterations of the inner loop body of `wide_1()`?  Based on the dependences in the inner loop, estimate the CPI for this code (assuming the inner loop runs many, many times).        \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "1. **critical path**:\n",
    "    \n",
    "2. **CPI**:\n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\"> \n",
    "\n",
    "Here's a drawing of of dependences for two iterations (first iteration in red, the second in blue).\n",
    "    \n",
    "![image.png](img/wide_1_dep.png)\n",
    "\n",
    "\n",
    "All the `addq` instructions in the body read and write `rbx`, so each is dependent on the one prior to it, and there is a cross-loop dependence on `rbx` as well.\n",
    "    \n",
    "Further, the `cmpq` is dependent on the last `addq` as well, and the `jae` is dependent on the `cmpq`. However, they don't add to the critical path for two reasons:\n",
    "    \n",
    "1. Nothing is dependent on the `jae`\n",
    "2. The `jae` is a very predictable branch, so it and the `cmpq` will not affect execution very much.  Likewise, we can ignore the `jmp` as well.\n",
    "    \n",
    "So, the critical for $n$ iteration is $6n$.\n",
    "\n",
    "The nominal latency for an `addq` is 1 cycle, so the latency of $n$ iterations is about $6n$ cycles.    There are 9 instructions per iteration, so `CPI` should be 6/9 = 0.66.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's see how it comes out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"ilp.cpp\", function=[\"wide_1\"], name=\"ilp\", opt=\"-O0\", analyze=False, run=[\"perf_count\"], \n",
    "           cmdline=f\"--size 10000000\", \n",
    "           perf_cmdline=\"--stat-set PE.cfg --MHz 3500\")\n",
    "\n",
    "render_csv(\"ilp.csv\", columns=[\"function\", \"IC\", \"CPI\", \"CT\", \"ET\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Pretty good agreement!\n",
    "\n",
    "Let's try this function.  Same questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"ilp.cpp\", function=[\"wide_2\"], name=\"ilp\", opt=\"-O0\", run=[], \n",
    "           cmdline=f\"--size 10000000\", \n",
    "           perf_cmdline=\"--stat-set PE.cfg --MHz 3500\")\n",
    "compare([t.source, t.cfg])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 2,
    "cse142L.question_type": "correctness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question correctness points-2\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "What's the critical path length through $n$ iterations of inner loop body of `wide_2()`?  Based on the dependences in the inner loop, estimate the CPI for this code (assuming the inner loop runs many, many times).        \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "1. **critical path**: \n",
    "    \n",
    "2. **CPI (show your work)**: \n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"ilp.cpp\", function=[\"wide_2\"], name=\"ilp\", opt=\"-O0\", analyze=False, run=[\"perf_count\"], \n",
    "           cmdline=f\"--size 10000000\", \n",
    "           perf_cmdline=\"--stat-set PE.cfg --MHz 3500\")\n",
    "\n",
    "render_csv(\"ilp.csv\", columns=[\"function\", \"IC\", \"CPI\", \"CT\", \"ET\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "I have three more of these functions: `wide_3()` through `wide_5()` with more parallel sequences of dependent increments: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"ilp.cpp\", show=\"wide_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's see how they compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"ilp.cpp\", function=[\"wide_1\", \"wide_2\", \"wide_3\", \"wide_4\", \"wide_5\"], name=\"ilp\", opt=\"-O0\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size 10000000\", \n",
    "           perf_cmdline=\"--stat-set PE.cfg --MHz 3500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "display(render_csv(\"ilp.csv\", columns=[\"function\", \"IC\", \"CPI\", \"ET\"]))\n",
    "plotPEBar(\"ilp.csv\", what=[(\"function\", \"CPI\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Wow, we came very close to hitting our theoretical minimum CPI!:  The processor can issue up to 4 arithmetic instructions per cycle and we got a CPI of just about 0.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Dynamic Loop Unrolling\n",
    "\n",
    "`wide_5()` is not a very realistic function, since it has an artificially huge amount of ILP.  But ILP can arise in more natural ways as well.\n",
    "\n",
    "Below is a function that represents a common pattern:  A loop body that computes a value based on the loop index and stores it somewhere.  In this case, the computations is a long chain of additions.   Study it and answer the question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"unroll.cpp\", function=\"unroll_1\", opt=\"-O0\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* unroll_1(uint64_t threads, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    register uint64_t a = 4;\n",
    "\n",
    "    for(register uint64_t i = 0; i < size; i++) {\n",
    "       a = i+1;\n",
    "       a = a+1;       \n",
    "       a = a+1;       \n",
    "       a = a+1;       \n",
    "       a = a+1;       \n",
    "       data[i] = a;\n",
    "    }\n",
    "    data[0] = a;\n",
    "    return data;\n",
    "}\n",
    "FUNCTION(one_array_2arg, unroll_1);\n",
    "\"\"\", run=[], \n",
    "           cmdline=f\"--size 10000000\", \n",
    "           perf_cmdline=\"--stat-set PE.cfg --MHz 3500\")\n",
    "display(t.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "What's the critical path length through $n$ iterations of inner loop body of `unroll_1()` (assume $n$ is large)?  Estimate the CPI for this code (assuming the inner loop runs many, many times, and the latency for each instruction is 1).        \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "1. **critical path**:\n",
    "    \n",
    "2. **CPI**:\n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "    \n",
    "Here's a sketch of the data dependences in this loop across 4 iterations.  The instructions are abbreviated to their first letter:\n",
    "\n",
    "![image.png](img/unrolled_dependences.png)\n",
    "    \n",
    "The critical path through one loop iteration is 6 instructions.  The key is that while there is a cross iteration dependence on `r12` that dependence chain is very short, so the processor can start executing a new iteration of the loop  (i.e., compute a new value of `r12`) every cycle.  That means that many iterations of the loop will be \"in flight\" at once.\n",
    "\n",
    "For large $n$, the critical path is $n$ (for the increments on `r12`) plus the critical path through last iteration of the loop.  In the example above this is 9 (3 increments of `r12` plus 6 instructions in last iteration).  In general, it would $n + 6$.\n",
    "    \n",
    "There are 13 instructions per iteration.  So, a first cut at CPI would be $(n + 6)/13n$.  For large, $n$ this becomes 1/13 = 0.07, which would be amazing!   However, it's not to be:  The loop is all ALU instructions plus one store, and the CPU can only issue 4 ALU instructions per cycle.  The store will add a little more parallelism, so CPI can't be much better than 1/4 = 0.25.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"unroll.cpp\", function=\"unroll_1\", opt=\"-O0\", analyze=False, run=[\"perf_count\"], \n",
    "           cmdline=f\"--size 10000000\", \n",
    "           perf_cmdline=\"--stat-set PE.cfg --MHz 3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_csv(\"build/unroll.csv\",columns=[\"function\", \"IC\", \"CPI\", \"CT\", \"ET\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "That falls a short of our expectation.  Figuring out why would require some careful investigation, but there are a few complications the analysis above didn't consider:\n",
    "\n",
    "1.  The load from `var_20h` is 4 cycles, so the path from that instruction to store is 5 cycles, which is _almost_ the critical path.  Unlucky scheduling could increase the latency further.\n",
    "2.  The load, store, and `lea` instructions can only issue subsets of the functional units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.question_type": "optional",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question optional\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Copy the code for `unroll_1()` to create new function called `unroll_byhand()`.  Can you unroll the loop by hand and get better performance than the processor can unrolling it in hardware?    \n",
    "\n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "1. **critical path**: \n",
    "    \n",
    "2. **CPI (show your work)**: \n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Memory-level Parallelism\n",
    "\n",
    "One of the key themes in this class is that it's really all about memory, so it is with ILP as well:  Parallelism among arithmetic instructions is fine, but it's the parallelism among memory operations that really pays off.  \n",
    "\n",
    "In fact memory parallelism is so important that it has its own name: Memory Level Parallelism (MLP).\n",
    "\n",
    "Let's take a look at a new and improved miss machine and use it to explore MLP.\n",
    "\n",
    "## Miss Machine++\n",
    "\n",
    "Our new miss machine is much like the old one except it's wrapped up in a class so we can easily create more than one of them and pass them around as objects.  It also has support for performing stores in addition to loads as it walks around the links.   Look over the code and make sure you understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"MissMachine.hpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Parallel Miss Machines\n",
    "\n",
    "We will use `MissMachine` to create parallel chains of loads much like we built parallel chains of adds in the previous section.\n",
    "\n",
    "The code below accomplishes this by creating a miss machine, and then spreading four pointers uniformly along the circular chain of links.  Then we traverse the chain at all four points in the chain in parallel.  Here's the code and the assembly (sorry, the CFG tool breaks on this code for some reason).  The key basic block is the three instructions after `.L196`, `al` and `bl` are in `rax` and `rbx`.\n",
    "\n",
    "![image.png](img/miss_machine_loop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "#key data_cell\n",
    "miss_machines = fiddle(fname=\"miss_machines.cpp\", function=\"sample\", opt=\"-O3\", number_nodes=True, run=[])\n",
    "compare([miss_machines.source, miss_machines.asm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 4,
    "cse142L.question_type": "correctness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question correctness points-4\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "What's the critical path (in cycles) for the inner loop above?  And what's the estimated CPI.  Recall that the latency for a cache hit on our machine is 4 cycles.  Assume the loads all hit. </div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "1. **critical path**:  \n",
    "    \n",
    "2. **CPI**:  \n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### L1 Hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Our goal is to measure our processor's L1 cache bandwidth by maximizing memory-level parallelism.  To do this, `miss_machines.cpp` has a function with a large switch statement that starts like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"miss_machines.cpp\", show=(\"//START\", \"//END\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Each `case` runs the same loop with a different number \"miss chains\" -- parallel sequences of loads.  You can look as the assembly if you want, but it's long and messy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_code(\"build/miss_machines.s\", lang=\"gas\", show=\"miss_machines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.question_type": "optional",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question optional\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "How did the compiler implement the switch statement?  Is it faster than a sequence of `if-else` statements?\n",
    "    \n",
    "</div>\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "As we add more parallel chains, bandwidth will increase, but only to a point.\n",
    "\n",
    "![image.png](img/issue_ports.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "Based on the diagram above, how many parallel miss chains will produce the maximum L1 bandwidth (most loads completed per second)?  What do you think that bandwidth will be?  With that number of miss chains, what will the CPI be?  Remember: Load hits take 4 cycles.\n",
    " </div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "**How many miss chains**: \n",
    "    \n",
    "**L1 Bandwidth**:  \n",
    "    \n",
    "**CPI**:\n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "The diagram shows two load units, which suggests that the processor execute two loads per cycle.  We'll call this the maximum _issue rate_.  This would seem to suggest that 2 miss chains would be sufficient.  They would run at 3.5GHz giving us $7\\times 10^9$ loads of 8 bytes per second or 56GB/s.  \n",
    "    \n",
    "However, there is a catch:  Each load takes 4 cycles.  So it's not so much that the processor can \"perform\" two loads at a time, but that it can _start_ two loads at a time.  Those loads will finish 4 cycles later.  In the three intervening cycles, the processor could start 6 more loads.\n",
    "    \n",
    "This suggests that we will need 8 miss chains to achieve our 56GB/s.\n",
    "\n",
    "To compute the CPI, we can look at the loop body for `mm_sample()`:\n",
    "\n",
    "```gas\n",
    ".L194:\n",
    "\tmovq\t(%rax), %rax\n",
    "\tmovq\t(%rbx), %rbx\n",
    "\tsubl\t$1, %edx\n",
    "\tjne\t.L194\n",
    "```\n",
    "\n",
    "Two loads -- one per miss chain -- and 2 more instructions.  With 8 chains, that'd be 10 (1 per miss-chain + 2 more) instructions per loop iteration with a critical path of 4 cycles (The latency of 1 load), so that'd be a CPI of 0.4.\n",
    "\n",
    "If you dig through the longer assembly list above, you'll find that is not quite right.  Here's the loop body for 8 miss-chain loop:\n",
    "    \n",
    "```gas\n",
    "    \n",
    ".L122:\n",
    "\tleal\t1(%rbx), %r13d\n",
    "\tmovq\t(%rax), %rax\n",
    "\tmovq\t(%rdx), %rdx\n",
    "\tmovq\t(%rsi), %rsi\n",
    "\tmovq\t(%rdi), %rdi\n",
    "\tmovq\t%r13, %rbx\n",
    "\tmovq\t(%r8), %r8\n",
    "\tmovq\t(%r12), %r12\n",
    "\tmovq\t0(%rbp), %rbp\n",
    "\tmovq\t(%r11), %r11\n",
    "\tcmpq\t%r15, %r13\n",
    "\tjb\t.L122\n",
    "```\n",
    "\n",
    "It has 13 instructions (I'm not sure what the extra three instructions are for), which would make the CPI 4/13=0.31.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's see how it comes out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "t = fiddle(fname=\"miss_machines.cpp\", function=\"miss_machines\", name=\"L1_hit_machine\", opt=\"-O3 \", analyze=False, run=['perf_count'], \n",
    "           cmdline=f\"--size {4*4096} --arg1 8 --arg2 1 2 3 4 5 6 7 8 9 --arg3 10000000\", \n",
    "           perf_cmdline=f\"--stat-set L1.cfg --MHz 3500  --calc GB_per_sec=arg3*arg2*8/ET/{1024*1024*1024}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "display(render_csv(\"L1_hit_machine.csv\", columns=[\"size\", \"arg1\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\", \"GB_per_sec\"]))\n",
    "plotPE(\"L1_hit_machine.csv\", lines=True, what=[(\"arg2\", \"CPI\"),(\"arg2\", \"GB_per_sec\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Pretty good! \n",
    "\n",
    "So, our processor can handle (at least) at 8 outstanding loads.  Recall that earlier, the lab asked whether the processor cared more about (i.e., could exploit more) parallelism between arithmetic operation or memory operations.  What did you answer?  What do you think now?\n",
    "\n",
    "Executing loads from parallel miss chains simultaneously allows the processor to \"hide\" memory latency:  Even though each load takes 4 cycles, running them in parallel makes it appear that each takes just 0.33 cycles on average.\n",
    "\n",
    "It's nice to hide the 4-cycle L1 hit time, but, as you learned from the last two labs, L1 hits are not where the real problems lie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.question_type": "optional",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question optional\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Edit `miss_machines.cpp` to add another `case` that minimizes CPI.  How low can you go?\n",
    "\n",
    "</div>\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### L1 Misses\n",
    "\n",
    "Let's see about hiding the latency of some L1 misses that hit in the L2.  To do this, we'll just expand the size of our miss machine to fit in the L2 but not the L1.  THe L2 is 256kB, so 128kB should do it.  We'll also increase the link size to 64 bytes so we don't have any cache line reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "t = fiddle(fname=\"miss_machines.cpp\", function=\"miss_machines\", name=\"L1_miss_machines\", opt=\"-O3 \", analyze=False, run=['perf_count'], \n",
    "           cmdline=f\"--size {128*1024} --arg1 64 --arg2 1 2 3 4 5 6 7 8 9 10 11 12 13 --arg3 10000000\", \n",
    "           perf_cmdline=f\"--stat-set L1.cfg --MHz 3500  --calc GB_per_sec=arg3*arg2*8/ET/{1024*1024*1024}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cse142L.is_response": true,
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "display(render_csv(\"L1_miss_machines.csv\", columns=[\"size\", \"arg1\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\", \"GB_per_sec\"]))\n",
    "plotPE(\"L1_miss_machines.csv\", lines=True, what=[(\"arg2\", \"CPI\"), (\"arg2\", \"GB_per_sec\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Answer the questions below:\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"answer\">\n",
    "    \n",
    "**How many outstanding cache misses can the processor handle?**\n",
    "        \n",
    "**What's the peak bandwidth?**\n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "    \n",
    "**How many outstanding cache misses can the processor handle?**   In the experiment we increase the amount of MLP _in our program_ from 1 to 13 parallel load chains.  When the number of parallel load chains exceeds the number of parallel loads the processor can handle, performance stops improving (i.e., CPI stops dropping) because performance doesn't benefit from the additional parallelism.  Put another way, the program now has more ILP than the hardware can make use of.  For the data I got for this experiment this happened with 10 miss chains, so I conclude that the number of outstanding misses the processor can handle is 10.\n",
    "        \n",
    "**What's the peak bandwidth?**  This is just the highest point in the graph on the right.  For me it was about 21.8GB/s.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's go further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### L2 Misses\n",
    "\n",
    "Same drill:  We'll increase the miss machine size so it fit's in the L3 but not the L2:  The L3 is 12MB, so 2MB should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "t = fiddle(fname=\"miss_machines.cpp\", function=\"miss_machines\", name=\"L2_miss_machines\",\n",
    "           opt=\"-O3 \", analyze=False, run=['perf_count'], \n",
    "           cmdline=f\"--size {2*1024*1024} --arg1 64 --arg2 1 2 3 4 5 6 7 8 9 10 11 12 13 --arg3 100000000\", \n",
    "           perf_cmdline=f\"--stat-set L2.cfg --MHz 3500  --calc GB_per_sec=arg3*arg2*8/ET/{1024*1024*1024}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "display(render_csv(\"L2_miss_machines.csv\", columns=[\"size\", \"arg1\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L2_MPI\", \"GB_per_sec\"]))\n",
    "plotPE(\"L2_miss_machines.csv\", lines=True, what=[(\"arg2\", \"CPI\"), (\"arg2\", \"GB_per_sec\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Still 10 misses, but just over 6GB/s and an average latency of 3.2 cycles, and the L3 can do about 5.8GB/s.\n",
    "\n",
    "We are going to stop there.  The next step would be to do L3 misses, but it would take a lot more work to get meaningful results for DRAM, and the results are strongly dependent on how our systems are configured, which we discussed (and measured the bandwidth of in Lab 3).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Thread Performance\n",
    "\n",
    "ILP and MLP exist within a single core, and the degree of parallelism is limited by the number of instructions the CPU can issue per cycle.  To get more parallelism, we need to use more CPUs, and for that we'll need to create threads.\n",
    "\n",
    "A thread is a flow of control through your program that runs on a processor.  Every program has at least one thread, and by creating multiple threads you can spread the work of your program across many cores, hopefully improving it's performance.\n",
    "\n",
    "Making programs fast via multi-threading is an extremely deep and complex area.  We could easily spend an entire quarter studying techniques for creating, managing, and using threads, and most universities (including UCSD) offer several courses on this topic (Start with Operating Systems and then take the graduate Parallel Computation course).  Indeed, some people have devoted their entire careers to the topic.  \n",
    "\n",
    "All this effort is for good reason:  the amount of ILP and MLP that individual cores can utilize has been roughly constant over last decade and shows no signs of improving much.  Making matters worse, clock speeds are growing very slowly.  That means that adding cores is the main way that computer are getting faster, but that only works if we can use threads effectively.\n",
    "\n",
    "But, it's week 8, and we don't have time for all of that.  Instead, we are going to take a whirlwind tour of how you can create threads, how to make them communicate with one another, why the underlying hardware can make that hard and/or slow, and what you can do about it.\n",
    "\n",
    "To start, let's see how many cores we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!cse142 job run --lab parallel --force 'lscpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The key lines are `Socket(s): 1` and `Core(s) per socket:  6`.  A \"socket\" is place on a motherboard to stick a physical CPU.  We have 1, so all our cores live on one chip. That chip has 6 \"cores\".  A core is complete processor pipeline. \n",
    "\n",
    "You might notice that it also says `CPU(s): 12`.  This would be better phrased as \"logical cores\".  It's twice the number of actual (physical) cores because each of the cores can run two threads at once via Hyperthreading.  These cores are numbered 0-11.\n",
    "\n",
    "By convention, the lower half of the core numbers cover one logical core on each physical core.  The top half of the numbers are the second logical cores.  So, logical core 0 and logical core 6 are on the same physical core.\n",
    "\n",
    "For now, we are going to stick to logical cores 0-5.  We'll return to the upper 6 logical processors later in the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Spawning Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The first step to using threads is to create some.  C++ has pretty good threading facilities.  The key is the `std::thread` object that represents a running thread.  To start a thread, you create an `std::thread` object and pass it a function to call and the arguments you'd like to pass to the function.\n",
    "\n",
    "The `std::thread`'s `join()` method waits for the thread to complete.\n",
    "\n",
    "Here's some code that runs three threads that print out some numbers. The cell below will run the code three times separated by \"FINISHED EXECUTION\".  Pay close attention to the output of each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"threads.cpp\", function=[\"threads\", \"threads\", \"threads\"], analyze=False,  opt=\"-O0\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "\n",
    "void go() {\n",
    "    for(int i = 0; i < 10; i++) \n",
    "        std::cerr << i << \"\\n\";\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* threads(uint64_t threads, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    std::thread t1(go);  // Create a thread to run go().  Pass no arguments.\n",
    "    std::thread t2(go);\n",
    "    std::thread t3(go);\n",
    "    \n",
    "    t1.join(); // wait for t1 to finish.\n",
    "    t2.join();\n",
    "    t3.join();\n",
    "    std::cerr << \"FINISHED EXECUTION\\n\";\n",
    "    return data;\n",
    "}\n",
    "FUNCTION(one_array_2arg, threads);\n",
    "\"\"\", run=[\"perf_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "See how the order of the number changes?  This is because the relative execution rate of each thread is different.  Also, the threads take turns writing to standard output and the order they go in is non-deterministic.\n",
    "\n",
    "This non-determinism is the bane of multi-threaded debugging:  Imagine if your bug only occurred for one of the very many possible orderings of the operations in your threads?  Your bug might occur just 1 in 100 (or 1000 or 10,000) times you run the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Measuring Thread Behavior\n",
    "\n",
    "Of course, we will want to measure the performance and behavior of our threads.  Things get a little tricky here, because of a limitation of our performance counter measurement library.  It can only measure performance counters of the main thread of the program.  \n",
    "\n",
    "For the experiments we are going to run this is not a big problem: All our threads will be doing the same thing at the same time, so measuring one is a good as measuring any other.  In some cases, though, we will need to _estimate_ aggregate values across all the cores/threads.  For instance, if we want to estimate the _total_ number of instructions execute by all threads, we'll multiply the single-thread IC we measure for one thread by the thread count.\n",
    "\n",
    "Here's a simple threaded program that runs one miss chain in each of several threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"threads.cpp\", function=\"threads\", analyze=False,  opt=\"-O3\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "#include\"MissMachine.hpp\"\n",
    "\n",
    "void go(MissMachine * machine, uint64_t arg2) {\n",
    "    machine->load_miss(arg2);\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* threads(uint64_t thread_count, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    MissMachine a( arg1, size);\n",
    "    a.make_links();\n",
    "\n",
    "    std::thread **threads = new std::thread*[thread_count-1]; // Alloce space for some pointers\n",
    "    for(unsigned int i = 0; i < thread_count-1; i++) {\n",
    "        threads[i] = new std::thread(go, &a, arg2); // create the threads.  They will each run the miss machine.\n",
    "    }\n",
    "    go(&a, arg2); // So will this thread, hence the -1\n",
    "    for(unsigned int i = 0; i < thread_count-1; i++) { // wait for everyone else\n",
    "        threads[i]->join();\n",
    "        delete threads[i]; // cleanup\n",
    "    }\n",
    "    delete threads; // cleanup.\n",
    "    \n",
    "    return data;\n",
    "}\n",
    "FUNCTION(one_array_2arg, threads);\n",
    "\"\"\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size 4096 --arg1 8 --arg2 100000000 --thread 1 2 3 4 5 6\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "#key data_cell\n",
    "display(render_csv(\"build/threads.csv\", columns=[\"thread\", \"size\", \"arg1\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\"]))\n",
    "plotPE(\"build/threads.csv\", lines=True, what=[(\"thread\", \"IC\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "correctness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question correctness points-1\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "With 6 threads how many total instructions were executed?\n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "**Total IC**: \n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Thread Communication with Volatile Variables and Locks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "In order for threads to work together, they must share variables:  _Sharing_ means that more than one thread is reading and/or writing to the variable during the same period of time.  Threads working together _must_ share some information, otherwise they cannot make progress together on a common goal.  For example, imagine expecting 8 people in sealed rooms, who have never met, to make progress on a single task -- it is not possible.\n",
    "\n",
    "There are two separate problems we need to solve.  The first is _how_ to share data and the second is how to share it in a coordinated and reliable way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Sharing Data Between Threads with `volatile`\n",
    "\n",
    "The code below declares a global variable, initializes it to zero, and then provides `wait()` to wait for it to change.  It also provides `signal()` to update the global variable.  You could imagine that two threads could use these two function to coordinate in a simple way:  Thread `T1` could call `wait()` to wait for another thread, `T2`, to do something.  When `T2` is done, it could call `signal()` to let `T1` know it has done it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"not_shared.cpp\", function=\"wait\", analyze=True,  opt=\"-O3\",\n",
    "code=r\"\"\"\n",
    "\n",
    "int flag = 0;\n",
    "extern \"C\"\n",
    "void wait() {\n",
    "    while(flag);\n",
    "}\n",
    "\n",
    "void signal() {\n",
    "    flag = 1;\n",
    "}\n",
    "\"\"\", run=None)\n",
    "t.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "As written and compiled with optimizations, the compiler does what we would expect:  it checks `flag` once. If it's non-zero (i.e., it evaluates to `true`), it returns.  Otherwise it loops infinitely.  This will clearly not work as a thread communication mechanism:  Unless `T2` calls `signal()` _before_ `T1` calls `wait()`, `T1` will never get the message.\n",
    "\n",
    "The compiler is assuming that `flag` _will not change_.  This is a valid assumption for the compiler to make because, by default, variables in C and C++ are considered to the thread-private -- only the current thread will access them.  That's not what we want for thread communication.\n",
    "\n",
    "We can fix this by declaring `flag` as `volatile`.  `volatile` tells the compiler that the variable is shared and so it might change at any time, which dramatically reduces the number of optimizations it can apply.  Let's see what it does now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"not_shared.cpp\", function=\"wait\", analyze=True,  opt=\"-O3\",\n",
    "code=r\"\"\"\n",
    "\n",
    "volatile int flag = 0;\n",
    "extern \"C\"\n",
    "void wait() {\n",
    "    while(flag);\n",
    "}\n",
    "\n",
    "void signal() {\n",
    "    flag = 1;\n",
    "}\n",
    "\n",
    "\"\"\", run=None)\n",
    "t.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Now, `wait()` checks `flag` every time, and our communication mechanism will work.  Or will it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Memory Ordering\n",
    "\n",
    "Here's a slightly more complicated example that actually uses our communication mechanism: instead of waiting on a fixed value for flag, `wait()` will wait for a configurable value.  The threads take turns waiting on one another, and set `other_value` each time.  They also check whether `flag` and `other_value` match, and tell us if they don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"toggle.cpp\", function=\"toggle\", analyze=False,  opt=\"-O3\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "\n",
    "volatile int flag = 0;\n",
    "volatile int other_value = 0;\n",
    "\n",
    "extern \"C\"\n",
    "void wait(int k) {\n",
    "    while(flag != k);\n",
    "}\n",
    "\n",
    "void signal(int k) {\n",
    "    flag = k;\n",
    "}\n",
    "\n",
    "void play(int my_id, int other_id, int count) {\n",
    "    for(int i = 0; i < count; i++) {\n",
    "        wait(other_id);\n",
    "        int t_flag = flag;\n",
    "        int t_other_value = other_value;\n",
    "        if (t_flag != t_other_value) {\n",
    "            std::cerr << \"Mismatch: \" << t_flag << \" != \" << t_other_value << \"\\n\";\n",
    "        }\n",
    "        other_value = my_id;\n",
    "        signal(my_id);\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* toggle(uint64_t thread_count, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    std::thread T0(play, 0, 1, 10000000);\n",
    "    std::thread T1(play, 1, 0, 10000000);\n",
    "    \n",
    "    T0.join();\n",
    "    T1.join();\n",
    "    return data;\n",
    "}\n",
    "\n",
    "FUNCTION(one_array_2arg, toggle);\n",
    "\"\"\", run=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "In the above code, will the `std::cerr` line ever execute?  Why or why not?\n",
    "\n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "    \n",
    "We start off with `flag == 0` and `other_value == 0`.\n",
    "    \n",
    "`T0` will spend some time `wait()`ing for 1 (`other_id`).\n",
    "    \n",
    "`T1` will call `wait()` for 0 (its value of `other_id`), but will not wait at all, since `flag` was initialized to 0.  `T1` will read `flag` and `other_value`.  They will both be 0, and the `std::cerr` will not happen.\n",
    "    \n",
    "`T1` then sets `other_value` to 1 (it's value of `my_id`).  *After* that, it calls `signal()` which sets `flag` to 1.\n",
    "    \n",
    "At this point, it seems that `T0`'s call to `wait()` will find that `flag == 1`, and stop waiting.  Then `T0` will the do the same things that `T1` did, as described above.\n",
    "\n",
    "It would seem that since each thread sets `other_value` before calling `signal()`, that the `std::cerr` should never execute.\n",
    "    \n",
    "But of course, then, why did I write this code and this question?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's run it and see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"toggle.cpp\", function=\"toggle\", analyze=True,  opt=\"-O3\", run=[\"perf_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Hmmm... That is definitely not \"never\" executing, although it is pretty rare.  It's also non-deterministic:  If you run it more than once, you'll get different mismatches.\n",
    "\n",
    "What's going on?  It would seem that our intuitive understanding of how our multi-threaded program should execute does not match what actually happens when it really executes.\n",
    "\n",
    "This source of this mismatch is something called the processor's _consistency model_.   The consistency model determines how processors \"see\" memory operations performed by other processors.  In particular, it  determines in what order stores _appear_ to occur.  Different instruction sets come with different consistency models: x86 has one, ARM has one, etc.\n",
    "\n",
    "Consistency models are easily the most complicated aspect of modern Instruction Set Architectures, and their details are pretty far beyond the scope of this course. However, they are crucial for making multithreaded programs work, so we will cover a few basic points.\n",
    "\n",
    "The basic problem with the code above is that the x86 consistency model allows the stores that `T0` performs to _appear_ , from `T1`'s perspective, to happen in a different order than `T0` executed them in.  In the code above, `T1` can load from `flag` and find that it's equal to 0 _and then_ read from `other_value` and find it equal to one.\n",
    "\n",
    "x86 provides some special instructions that prevent this reordering.  They are called 'fences' because they keep memory operations from moving around.  It's possible to use them directly, but that's a subject for another class.  Instead, we'll use _locks_, which are a cleaner way of coordinating threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "###  Locks\n",
    "\n",
    "A _lock_ or _mutual exclusion variable (mutex)_ is a small data structure that can be _locked_ and _unlocked_.  We'll use C++'s `std::mutex`.  \n",
    "\n",
    "If a thread calls `lock()` on a mutex that is not currently locked, the thread \"holds\" the lock and starts executing a region of code called a \"critical section\".  At the end of the critical section, it calls `unlock()` to release the lock.\n",
    "\n",
    "If a thread, `T`, calls `lock()` on a mutex that _is_ currently locked, it will wait until the thread that holds it calls `unlock()`.  Then, `T` gets the lock and can proceed.\n",
    "\n",
    "The result is that, at any time, only one thread is executing inside the critical section.\n",
    "\n",
    "Internally, locks are implementing using some kind of flag (similar to `flag` in our example above) and some of those fences I mentioned above. \n",
    "\n",
    "Here's an example.  In this code, we have a shared variable `shared`.  Several threads are going to work together to increment `shared` 600,000 times.  If we run with $n$ threads, each will do 600,000/$n$ increments.\n",
    "\n",
    "If `do_lock` is `true`, they will use `yes_locks()` which protects each lock with an increment.  Otherwise, they will use `no_locks()` which doesn't.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"lock_demo.cpp\", function=\"lock_demo\", analyze=False,  opt=\"-O1\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "#include<mutex>\n",
    "\n",
    "std::mutex lock;\n",
    "volatile int shared = 0;\n",
    "extern \"C\"\n",
    "void yes_locks(uint64_t id, int count) {\n",
    "    for(int i= 0; i < count; i++){\n",
    "        lock.lock();\n",
    "        shared++;\n",
    "        lock.unlock();\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "void no_locks(uint64_t id, int count) {\n",
    "    for(int i= 0; i < count; i++){\n",
    "        shared++;\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* lock_demo(uint64_t thread_count, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    shared = 0;\n",
    "    std::thread **threads = new std::thread*[thread_count];\n",
    "    bool do_locks = (arg2 != 0);\n",
    "    for(unsigned int i = 0; i < thread_count - 1; i++) {\n",
    "        if (do_locks) {\n",
    "            threads[i] = new std::thread(yes_locks, i, arg1/thread_count);\n",
    "        } else {\n",
    "            threads[i] = new std::thread(no_locks, i, arg1/thread_count);\n",
    "        }\n",
    "    }\n",
    "    if (do_locks) {\n",
    "        yes_locks(thread_count - 1, arg1/thread_count);\n",
    "    } else {\n",
    "        no_locks(thread_count - 1, arg1/thread_count);\n",
    "    }\n",
    "    for(unsigned int i = 0; i < thread_count - 1; i++) {\n",
    "        threads[i]->join();\n",
    "    }\n",
    "    std::cerr << \"do_lock: \" << do_locks << \"; \" << \"thread count: \" << thread_count << \"; Shared sum: \" << shared << \".  It should be: \" <<  arg1 << \"\\n\";\n",
    "    return data;\n",
    "}\n",
    "FUNCTION(one_array_2arg, lock_demo);\n",
    "\"\"\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--arg1 60000000 --arg2 1 0 --thread 1 2 3 4 5 6\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "As you can see from the output, we get the right answer when we use locks, and the wrong answer when we don't (except for 1-thread case).\n",
    "\n",
    "Here's the assembly for `no_locks()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "#key data_cell\n",
    "do_cfg(\"build/lock_demo.so\", symbol=\"no_locks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Based on the assembly/CFG above, and assuming multiple threads are running at once, explain how `shared` ends up being computed incorrectly without locks and how adding locks prevents it.\n",
    "  \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "        \n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "    \n",
    "The increment turns into 3 instruction:  `mov (%rcx), %eax` (a load), `addl $1, %eax` (the add), and `movl %eax, (%rcx)` (a store).\n",
    "\n",
    "The problem comes when the load in two threads retrieves the same value.  They then compute the same next value and store it back.  As a result one of the increments is lost.\n",
    "  \n",
    "Adding `lock()` and `unlock()` avoids this problem since only one thread is executing the increment at a time.  So no increments are lost.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "So locks are fine for correctness (which is very important), but what about performance.\n",
    "\n",
    "Here's a bunch of data and graphs about the performance of the code above with and without threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "#key data_cell\n",
    "from notebook import *\n",
    "df = render_csv(\"build/lock_demo.csv\")\n",
    "df[\"label\"] = df[\"thread\"].apply(lambda x: f\"{x} threads;\" ) + \" \" + df[\"arg2\"].apply(lambda x: \"locks\" if x else \"no locks\")\n",
    "df[\"Total IC\"] = df[\"IC\"] * df[\"thread\"]\n",
    "df[\"IC per increment\"] = df[\"IC\"]/(df[\"arg1\"]/df[\"thread\"])\n",
    "df[\"Cycles per increment\"] = df[\"cycles\"]/(df[\"arg1\"]/df[\"thread\"])\n",
    "df[\"L1 Misses Per Increment\"] = df[\"L1_cache_misses\"]/(df[\"arg1\"]/df[\"thread\"])\n",
    "df[\"locks\"] = df[\"arg2\"]\n",
    "display(df[[\"thread\", \"locks\", \"IC per increment\", \"CPI\", \"ET\", \"Cycles per increment\", \"L1 Misses Per Increment\"]])\n",
    "plotPEBar(df=df, include_numbers=False, what=[(\"label\", \"IC per increment\"), (\"label\", \"CPI\"), (\"label\", \"ET\"),  (\"label\", \"Cycles per increment\"), (\"label\", \"L1 Misses Per Increment\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 3,
    "cse142L.question_type": "correctness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question correctness points-3\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Answer the questions below:\n",
    "    \n",
    "\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "* How much does adding locks slow down the single thread case in terms of cycles per increment? \n",
    "    \n",
    "* How much does adding a second thread slow down each increment in terms of cycles per increment? \n",
    "    \n",
    "* How many cycles does it take to take and release a lock? \n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "There are two main things to take away from this data:\n",
    "\n",
    "1.  Just taking and releasing locks is expensive -- even if there's only one thread.  This overhead comes mostly from increased instruction count.  In addition, fences are expensive: up to 33 cycles on our machine.\n",
    "2.  When there is more than one thread competing for the lock, things get even worse.  This overhead comes from increase cache misses.\n",
    "\n",
    "Both of these are noteworthy.  The extra instructions in the locks are an example of the overhead involved in improving performance.  We saw this before with loop tiling:  Splitting and renesting loops can improve performance but it also increases instruction count, so some of the improved performance goes to paying for that overhead.  It the same thing here:  Taking and releasing a lock is code we didn't have to run before and that doesn't contribute to the useful work our program is doing.  There is no free lunch.\n",
    "\n",
    "The cache misses are a bigger problem and, interestingly, they are due to kind of cache miss we have not seen before in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Cache Coherence and the 4th C\n",
    "\n",
    "In the last two labs we've seen the Capacity, Conflict, and Compulsory misses, but there is a 4th kind of miss that only occurs in multi-processing systems:  Coherence Misses.\n",
    "\n",
    "As you learned in CSE142, cache coherence is how the processor keeps multiple caches synchronized, so that the processors all see the same value for a given address.\n",
    "\n",
    "To refresh your memory, the key point of coherence are:\n",
    "\n",
    "1.  Coherence operates on cache lines, like all things in the memory hierarchy.\n",
    "2.  Multiple processors can have a copy of the a cache line in their cache if they are _only_ reading from it.\n",
    "3.  Only a single processor may have a copy of the cache line if it is writing to it.\n",
    "\n",
    "Enforcing #2 and #3 is expensive:  When a processor wants to update a cache line, it has to tell all the processors that have a copy of it to _invalidate_ their copy.  This means removing it from the cache.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "When a memory access would have been a hit, but it is a miss because the cache was invalidated, we call that a _coherence miss_.\n",
    "\n",
    "Satisfying cache misses is multiprocessors is also more complicated due to coherence:  If a load misses in the cache it has to check the other caches to see if they have a copy.  If they do, then that copy is more up-to-date than what is in main memory, so that is where the cache line needs to be loaded from.  This is called a _cache-to-cache_ transfer.\n",
    "\n",
    "The invalidations and cache-to-cache transfers are implemented by sending message between the caches over an on-chip network. \n",
    "\n",
    "Here's an example of an on-chip network for an 18-core Intel Haswell processor:\n",
    "\n",
    "![image.png](img/haswell_onchip_network.png)\n",
    "\n",
    "Sending messages across such a network can be pretty expensive and take quite a long time.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Coherence Performance\n",
    "\n",
    "Here's a simple program to measure the cost of communicating between cores.  We are going to run two threads either on the same core or on two different cores.  The only trick is that we control which thread runs where.  You can look at `threads.hpp` to see how `bind_to_core()` works.\n",
    "\n",
    "One thing to keep in mind:  When the two threads are running on the same core, they will have to take turns, so that single core will be doing twice as much work than each core in the two-core case.\n",
    "\n",
    "So, in an ideal world, the two-core case would be twice as fast as the one-core case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"coherence.cpp\", function=\"coherence\", analyze=False, opt=\"-O3\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "#include<mutex>\n",
    "#include\"threads.hpp\"\n",
    "#include\"pthread.h\"\n",
    "\n",
    "std::mutex lock;\n",
    "volatile int shared = 0;\n",
    "void go(uint64_t id,int count) {\n",
    "    for(int i= 0; i < count; i++){\n",
    "        lock.lock();\n",
    "        shared++;\n",
    "        lock.unlock();\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* coherence(uint64_t thread_count, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    shared = 0;\n",
    "    std::thread other (go, 1, arg1);\n",
    "    bind_to_core(other, arg2);\n",
    "\n",
    "    bind_to_core(pthread_self(), arg3);\n",
    "    go(0, arg1);\n",
    "    other.join();\n",
    "    return data;\n",
    "}\n",
    "FUNCTION(one_array_2arg, coherence);\n",
    "\"\"\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--arg1 10000000 --arg2 0 1   --arg3 0\" , \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df = render_csv(\"build/coherence.csv\")\n",
    "df[\"other_core\"] = df[\"arg2\"]\n",
    "df[\"this_core\"] = df[\"arg3\"]\n",
    "df[\"label\"] = df[\"arg2\"].astype(str) + \" to \" + df[\"arg3\"].astype(str) \n",
    "df[\"IC per increment\"] = df[\"IC\"]/df[\"arg1\"]\n",
    "df[\"Cycles per increment\"] = df[\"cycles\"]/df[\"arg1\"]\n",
    "display(df[[\"thread\", \"size\", \"arg1\", \"this_core\", \"other_core\", \"arg3\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\"]])\n",
    "plotPEBar(df=df,  what=[(\"label\", \"CPI\"), (\"label\", \"ET\"), (\"label\", \"L1_cache_misses\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "In this case, two cores is not better than one:  Even with two processors to do the work, execution slows down by a wide margin!\n",
    "\n",
    "The underlying problem is all the coherence misses necessary as `shared`'s cache line \"ping pongs\" between the two cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Reducing Sharing\n",
    "\n",
    "Since frequent sharing seems to hurt performance, a good way to improve performance in concurrent programs is to reduce sharing.  In fact, a central tenant of multi-threaded programming to reduce sharing as much as possible.\n",
    "\n",
    "Here's a version of the code above with two changes:\n",
    "\n",
    "1. Most of the sharing removed:  Each thread works on its own variable and we add them up at the end.\n",
    "2. We don't use locks any more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cse142L.is_response": true,
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"false_sharing.cpp\", function=\"false_sharing\", analyze=False, opt=\"-O3\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "#include<mutex>\n",
    "#include\"threads.hpp\"\n",
    "#include\"pthread.h\"\n",
    "\n",
    "volatile int shared = 0;\n",
    "volatile int not_shared_0 = 0;\n",
    "volatile int not_shared_1 = 0;\n",
    "\n",
    "void go_0(uint64_t id,int count) {\n",
    "    for(int i= 0; i < count; i++){\n",
    "        not_shared_0++;\n",
    "    }\n",
    "}\n",
    "\n",
    "void go_1(uint64_t id,int count) {\n",
    "    for(int i= 0; i < count; i++){\n",
    "        not_shared_1++;\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* false_sharing(uint64_t thread_count, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    shared = 0;\n",
    "    std::thread other (go_0, 1, arg1);\n",
    "    bind_to_core(other, arg2);\n",
    "\n",
    "    bind_to_core(pthread_self(), arg3);\n",
    "    go_1(0, arg1);\n",
    "    other.join();\n",
    "    shared = not_shared_0 + not_shared_1;\n",
    "    return data;\n",
    "}\n",
    "FUNCTION(one_array_2arg, false_sharing);\n",
    "\"\"\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--arg1 10000000 --arg2 0 1   --arg3 0\" , \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "How much difference in performance do you expect to see between running both threads on one core vs. running them on two cores?\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df = render_csv(\"build/false_sharing.csv\")\n",
    "df[\"other_core\"] = df[\"arg2\"]\n",
    "df[\"this_core\"] = df[\"arg3\"]\n",
    "df[\"label\"] = df[\"arg2\"].astype(str) + \" to \" + df[\"arg3\"].astype(str) \n",
    "df[\"IC per increment\"] = df[\"IC\"]/df[\"arg1\"]\n",
    "df[\"Cycles per increment\"] = df[\"cycles\"]/df[\"arg1\"]\n",
    "display(df[[\"thread\", \"size\", \"arg1\", \"other_core\", \"arg3\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_MPI\"]])\n",
    "plotPEBar(df=df,  what=[(\"label\", \"CPI\"), (\"label\", \"ET\"), (\"label\", \"L1_cache_misses\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Interesting.  That helped some, but not as much we'd like:  Ideally, we'd get 2x speedup with 2 cores, but instead we only get about 1.4x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Add a single line to the code above to get the 2x performance improvement we seek. (Hint:  The memory system thinks in cache lines and so should you).\n",
    "    \n",
    "</div>\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "    \n",
    "The program above has no sharing in it, but it's written in terms of variables.  The memory hierarchy doesn't share variables, though, it shares cache lines.\n",
    "\n",
    "The problem here is that `not_shared_0` and `not_shared_1` reside in the same cache line, and that cache line is shared between the two processors, so there's still a lot of (now totally useless) communication going on between the two caches.\n",
    "    \n",
    "The solution is to add some padding between the those two variables.  An array of 8 `uint64_t`s should do it.  That will push them into two different cache lines.  Give it a try.\n",
    "   \n",
    "This phenomenon is called \"false sharing\" and it's quite common.  A great way to avoid false sharing among small objects is to make them cache line-aligned.  If only we had a way to control the alignment of memory objects we allocate...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Non-Uniform Memory Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Multiple processors complicates the notion of memory latency as well. \n",
    "\n",
    "If you look closely at the on-chip network above, you can notice that there are two memory controllers -- one on each of the side of the on-chip network.  This mean that, depending on where a core is in relation to the DRAM it's trying to access, the memory latency will be different.  Similarly, depending on which part of the L3 cache a cache line landed, the latency of an L2 cache might vary.\n",
    "\n",
    "This effect is called non-uniform memory access (NUMA).  NUMA effects are even greater if a computer has multiple sockets -- some memory requests will have to go between chips while others are \"local\".\n",
    "\n",
    "It'd be great to measure NUMA effects on our machine, but our machine only has 6 cores, 1 socket, and one (active) memory controller, so there's not enough non-uniformity to measure reliably.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Example: Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's apply what we have learned so far to implement a fast histogram.  A histogram counts the number of occurrences of data values in a sample of data.  In our case, we are going to count how often each of the 256 possible 1-byte values appears in an array of 64-bit values.  So our histogram will have 256 \"buckets\".  Our task is to compute the histogram as fast as possible.\n",
    "\n",
    "\n",
    "## Serial Histogram\n",
    "\n",
    "Here's a simple serial version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=\"unthreaded_histogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Run the cell below to see the CFG and assembly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"histogram.cpp\", function=\"do_unthreaded_histogram\", name=\"unthreaded\", opt=\"-O3\", analyze=False, run=[])\n",
    "do_cfg(\"build/histogram.so\", symbol=\"unthreaded_histogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The compiler has completely unrolled the inner loop and performs one increment for each byte in the 64-byte value.  Let's see how fast it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"histogram.cpp\", function=\"run_unthreaded_histogram\", name=\"histogram_unthreaded\", analyze=False, opt=\"-O3\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size {hist_size} --thread 0\" , \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\"], columns=hist_columns)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "display(hist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 3,
    "cse142L.question_type": "correctness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question correctness points-3\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Would this code benefit from tiling as a locality optimization?  Why or why not?\n",
    "\n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "\n",
    "    \n",
    "</div>\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Parallel Histogram\n",
    "\n",
    "Here's a simple threaded version.  It divides the input array into chunks and processes them in parallel.  We use a single lock to protect a shared set of buckets.\n",
    "\n",
    "This is very similar to what we did with tiling for cache locality, but here our focus is on dividing up the work across threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=(\"//START_THREADED\", \"//END_THREADED\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "How much speedup do you expect with 6 threads? Why?\n",
    "\n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "**Speedup:**\n",
    "\n",
    "**Why:**\n",
    "    \n",
    "</div>\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Run the two cells below to see how it performs.  Spoiler:  this takes a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"histogram.cpp\", function=\"run_threaded_histogram\",  name=\"histogram_threaded\", analyze=False, opt=\"-O3\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size {hist_size} --thread 1 2 3 4 5 6\" , \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_threaded.csv\"], columns=hist_columns)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"thread\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_cache_misses\"]*hist_data[\"thread\"]\n",
    "display(hist_data)\n",
    "plotPE(df=hist_data, lines=True, what=[(\"thread\", \"speedup\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The 0 threads data point is the baseline serial version.\n",
    "\n",
    "We have certainly not improved things:  The 1-threaded threaded version is about 10x slower than the unthreaded version.  With 6 threads, it's about 408x slower.\n",
    "\n",
    "Moneta can show what's causing the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Here are the accesses to data we are computing the histogram over: `data`.\n",
    "\n",
    "![image.png](img/hist1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Each color is a different thread.  You can see that they are all sequentially scanning their portion of the array.\n",
    "\n",
    "Now, let's take a look at the  counters:\n",
    "\n",
    "![image.png](img/hist2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "What a beautiful piece of abstract art!  Sharing is evident because each horizontal line has many colors in it.  Ideally, we would see horizontal lines of color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Finer-Grain Locks\n",
    "\n",
    "One problem is that we only have one lock even though we have 256 different buckets.  This means that each thread has to lock _all_ the buckets for each increment.  That's a lot of overhead and a lot of contention.  Let's fix by making our locks more \"fine grained\":  Each of them will cover just one bucket.\n",
    "\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=(\"//START_FINE\", \"//END_FINE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Run these two cells to see performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"histogram.cpp\", function=\"run_fine_locks_histogram\",  name=\"histogram_fine_locks\", analyze=False, opt=\"-O3\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size {hist_size} --thread 1 2 3 4 5 6\" , \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_fine_locks.csv\"], columns=hist_columns)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"thread\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_cache_misses\"]*hist_data[\"thread\"]\n",
    "display(hist_data)\n",
    "plotPE(df=hist_data, lines=True, what=[(\"thread\", \"speedup\"), ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "On the plus side, performance goes up after 2 threads, and 6 threads is about 1.6x faster than 2 threads.   On the down side, 6 threads is still 10x slower than the unthreaded case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Private Histograms\n",
    "\n",
    "We still have a lot of sharing.  Let's get rid of it by giving each thread its own set of buckets.  We'll allocate an array of `thread_count * 256` buckets and compute the index based on the thread's number and the byte's value.  We can also get rid of locks because there's no more sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=(\"//START_PRIVATE\", \"//END_PRIVATE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Run these two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"histogram.cpp\", function=\"run_private_histogram\",  name=\"histogram_private\", analyze=False, opt=\"-O3\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size {hist_size} --thread 1 2 3 4 5 6\" , \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_private.csv\"], columns=hist_columns)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"thread\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_cache_misses\"]*hist_data[\"thread\"]\n",
    "display(hist_data)\n",
    "plotPE(df=hist_data, lines=True, what=[(\"thread\", \"speedup\"), ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Things are better still.\n",
    "\n",
    "* 1 thread is only a little slower than the unthreaded version (because we have no locks).\n",
    "* 6 threads is 2.5x faster than 2 threads, which is pretty good since the best we could do is 3x.\n",
    "\n",
    "The drop at 2 threads is a problem, though because we never recover from it.\n",
    "\n",
    "Let's see what moneta shows now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Here's `histogram`:\n",
    "\n",
    "![image.png](img/hist3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "It looks like almost nothing because all there is the summing at the end.\n",
    "\n",
    "Here's a small part of the array of private counters:\n",
    "\n",
    "![image.png](img/hist4.png)\n",
    "\n",
    "Each horizontal line is a single color -- no more sharing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Private Histograms, Take 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "What causes the drop in performance from 1 thread to 2?  How can we fix it?\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "    \n",
    "</div>\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "It's false sharing due to this calculation:  `b*thread_count + id`\n",
    "    \n",
    "Computing the bucket to use this way arranges the buckets like this:\n",
    "    \n",
    "| 0 | 1 | 2 |3 |\n",
    "|--|--|--|--|\n",
    "|thread 0; byte == 0| thread 1; byte == 0| thread 2; byte == 0|...|--|--|\n",
    "\n",
    "Which puts buckets for multiple threads into the same cache line.\n",
    "\n",
    "We'd rather have this\n",
    "    \n",
    "| 0 | 1 | 2 |3 |\n",
    "|--|--|--|--|\n",
    "|thread 0; byte == 1| thread 0; byte == 1| thread 0; byte == 2|...|--|--|\n",
    "\n",
    "Which we can achieve with `id*256 + thread_count`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=(\"//START_PRIVATE2\", \"//END_PRIVATE2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's see how that does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"histogram.cpp\", function=\"run_private2_histogram\",  name=\"histogram_private2\", analyze=False, opt=\"-O3\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size {hist_size} --thread 1 2 3 4 5 6\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_private2.csv\"], columns=hist_columns+[\"cycles\"])\n",
    "#hist_data[\"ET\"] = hist_data[\"ET\"]/hist_data[\"thread\"].apply(lambda x: 1 if x == 0 else x)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"thread\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_cache_misses\"]*hist_data[\"thread\"]\n",
    "display(hist_data)\n",
    "plotPE(df=hist_data, lines=True, what=[(\"thread\", \"speedup\"), ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Yeah!  That's speedup!\n",
    "\n",
    "There's no longer any penalty for threading and 6 threads is almost exactly 6x faster!  Hooray!\n",
    "\n",
    "Let's see what moneta shows now:\n",
    "\n",
    "![image.png](img/hist5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Ah, big stripes of color!  No false sharing!  Hooray!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Amdahl's Law and Imperfect Speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "In our histogram example, we got 6x speedup with 6 threads.  Speeding up a program by $p$  with $p$ processors is called _linear speedup_ and it's always the goal of multi-threaded.  It's hard to achieve, however, because of Amdahl's Law.\n",
    "\n",
    "So far, in this class we've talked about Amdahl's Law as it applies optimizations:  The more widely applicable an optimization, the larger it's benefit.  Originally, however, Amdahl's Law was just about parallel computation.  In particular, if we have $p$ processors, Amdahl's Law bounds the maximum speedup, $S$, we can achieve:\n",
    "$$S \\leq \\frac{1}{x/p + (1-x)}$$\n",
    "\n",
    "Where $x$ is the fraction of the program that can parallelized across all $p$ processors.  For the histogram example, $x = 1$, so $S \\leq p$, and our implementation achieved this upper bound.\n",
    "\n",
    "Usually, however, $x < 1$, so $S < p$.\n",
    "\n",
    "To see this, let's parallelize merge sort.  The implementation below divides the array in half and recursively calls merge sort on each sub array.  To parallelize it, we will spawn a thread to sort each sub-array.  As the sub arrays get smaller and smaller, two things will happen:\n",
    "\n",
    "1.  We will spawn an enormous number of threads.\n",
    "2.  Each of them will do very little work.\n",
    "\n",
    "Neither of these is good, so we'll use `threshold` to control the minimum size for which we will spawn a thread.  If the sub-array is smaller than `threshold`, we'll just do all the work in the current thread.\n",
    "\n",
    "This means that if the array is of size `threshold` $ \\times 2^k$, we'll use a maximum of $2^k$ threads.\n",
    "\n",
    "You'll notice that there are no locks.  This is because there's not actually any sharing:  Only one thread is ever working on any one part of the array at a time.  (The attentive reader will recall that I said that sharing is necessary to coordinate threads.  Here, the sharing and coordination happen before the threads are created and after we `join()` them). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"merge_sort.cpp\", function=\"merge_sort\", analyze=False, opt=\"-Og\",\n",
    "code=r\"\"\"\n",
    "//  From https://codereview.stackexchange.com/questions/87085/simple-comparison-of-sorting-algorithms-in-c\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "#include<math.h>\n",
    "#include<mutex>\n",
    "#include\"threads.hpp\"\n",
    "#include\"pthread.h\"\n",
    "\n",
    "void merge(uint64_t *list, int64_t p, int64_t q, int64_t r)\n",
    "{\n",
    "//n1 and n2 are the lengths of the pre-sorted sublists, list[p..q] and list[q+1..r]\n",
    "\tint64_t n1=q-p+1;\n",
    "\tint64_t n2=r-q;\n",
    "//copy these pre-sorted lists to L and R\n",
    "\tuint64_t * L = new uint64_t[n1+1];\n",
    "\tuint64_t * R = new uint64_t[n2+1];\n",
    "\tfor(int64_t i=0;i<n1; i++)\n",
    "\t{\n",
    "\t\tL[i]=list[p+i];\n",
    "\t}\n",
    "\tfor(int64_t j=0;j<n2; j++)\n",
    "\t{\n",
    "\t\tR[j]=list[q+1+j];\n",
    "\t}\n",
    "\n",
    "\n",
    "//Create a sentinal value for L and R that is larger than the largest\n",
    "//element of list\n",
    "\tuint64_t largest;\n",
    "\tif(L[n1-1]<R[n2-1]) largest=R[n2-1]; else largest=L[n1-1];\n",
    "\tL[n1]=largest+1;\n",
    "\tR[n2]=largest+1;\n",
    "\n",
    "//Merge the L and R lists\n",
    "\tint64_t i=0;\n",
    "\tint64_t j=0;\n",
    "\tfor(int64_t k=p; k<=r; k++)\n",
    "\t{\n",
    "\t\tif (L[i]<=R[j])\n",
    "\t\t{\n",
    "\t\t\tlist[k]=L[i];\n",
    "\t\t\ti++;\n",
    "\t\t} else\n",
    "\t\t{\n",
    "\t\t\tlist[k]=R[j];\n",
    "\t\t\tj++;\n",
    "\t\t}\n",
    "\t}\n",
    "    delete L;\n",
    "    delete R;\n",
    "}\n",
    "\n",
    "void merge_sort_aux(uint64_t *list, int64_t p, int64_t r, int64_t threshold)\n",
    "{\n",
    "\tif(p<r)\n",
    "\t{\n",
    "        int64_t q=floor((p+r)/2);\n",
    "        if (r - p > threshold) {\n",
    "            std::thread left(merge_sort_aux, list,p,q, threshold);\n",
    "            std::thread right(merge_sort_aux, list,q+1,r, threshold);\n",
    "            left.join();\n",
    "            right.join();\n",
    "        } else {\n",
    "            merge_sort_aux(list,p,q, threshold);\n",
    "            merge_sort_aux(list,q+1,r, threshold);\n",
    "        }\n",
    "\t\tmerge(list,p,q,r);\n",
    "\t}\n",
    "\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* merge_sort(uint64_t thread_count, uint64_t *list, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "\tmerge_sort_aux(list, 0, size - 1, arg1);\n",
    "\treturn list;\n",
    "}\n",
    "FUNCTION(one_array_2arg, merge_sort);\n",
    "\n",
    "\"\"\", run=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 1,
    "cse142L.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"question completeness\">\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "What is $x$ for parallel merge sort?  Can you bound speedup for 4 processors, a threshold of 1024, and a total array size of 4096?  If you can't get a precise answer try to estimate or provide an upper bound the value of $x$.\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "    \n",
    "</div>\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "It's pretty tricky to calculate $x$ precisely, but we can do it big-O style.  The key observation is that merge sort in $O(n \\lg n)$ and the final merge operation is completely serialized.  That final merge takes $O(n)$ comparisons and assignments, so it takes $$\\frac{n}{n\\log n}$$ of the the total execution time.\n",
    "  \n",
    "So $x$ is something like:\n",
    "\n",
    "$$ x = 1 - \\frac{n}{n\\lg n} =  1 - \\frac{1}{\\lg n}$$\n",
    "    \n",
    "This tells that as $n$ get's really big, merge sort slowly approach being perfectly parallelizable (i.e., $x = 1$).  It won't ever get there, though.\n",
    "\n",
    "Let's plug it into Amdahl's law:\n",
    "    \n",
    "\n",
    "$$S \\leq \\frac{1}{\\frac{1 - \\frac{1}{\\lg n}}{p} + (1-(1 - \\frac{1}{\\lg n}))}$$\n",
    "\n",
    "And we have, $p = 4$, $n = 4$, so:\n",
    "\n",
    "$$S \\leq \\frac{1}{\\frac{1 - \\frac{1}{\\lg 4096}}{4} + (1-(1 - \\frac{1}{\\lg 4096}))}$$\n",
    "$$S \\leq \\frac{1}{\\frac{0.92}{4} + (1-(0.92))}$$\n",
    "$$S \\leq 3.2$$\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Given our implementation, we can't directly control the number of threads we are using, but by setting `threshold` to smaller values, we'll use more threads.  In particular, if we have `size == threshold * 2^k`, we will use a peak of `size/threshold` threads, that's what `approx threads` measures.\n",
    "\n",
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"merge_sort.cpp\", function=\"merge_sort\", analyze=False, opt=\"-Og\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size {8*1024*1024} --arg1 {8*1024*1024} {4*1024*1024} {2*1024*1024} {1024*1024}\", #\" {512*1024} {256*1024} {128*1024} {64*1024} {32*1024} {16*1024}\" , \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df = render_csv(\"build/merge_sort.csv\")\n",
    "df[\"i\"] = df.index\n",
    "df[\"threshold\"] = df[\"arg1\"]\n",
    "df[\"approx threads\"] = df[\"size\"]/df['threshold']\n",
    "df[\"speedup\"] = df.iloc[0][\"ET\"]/df[\"ET\"]\n",
    "display(df[[\"size\", \"threshold\", \"approx threads\", \"ET\", \"speedup\"]])\n",
    "plotPE(df=df, lines=True, what=[(\"approx threads\", \"speedup\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Speedup looks ok, but it's not  \"linear speedup\".  From 1 thread to 2, we get 2x, but from 2 to 4, we only get 1.56x.  From 4 to 8, we get 1.29 (although, in fairness, we only have 6 cores).\n",
    "\n",
    "Unfortunately, this how it usually goes:  Linear speedup is very hard to achieve in practice, because it requires reducing communication and serialized computation to nearly zero.  We were able to do that with the histogram, but sorting (and most other things) are more complex.\n",
    "\n",
    "And it's not just that $x$ is less than 1.  There is also usually some overhead from locks (which was absent in our histogram) which further degrades performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Hyperthreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Earlier in the lab, we saw that our CPU _says_ it has 12 processors even though it really only has 6.  The 6 extra, logical processors are due to hyperthreading, a clever trick that allows two threads to run _at exactly the same time_ on the same processor.\n",
    "\n",
    "The catch is that the two threads running on the same core compete with each other for resources.  This works fine if the two threads need different resources, but if they need the same resources, performance will suffer.\n",
    "\n",
    "Let's see how it does for our two parallel programs.  Here's the histogram with threads turned up to 12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"histogram.cpp\", function=\"run_private2_histogram\",  name=\"histogram_hyper\", analyze=False, opt=\"-O3\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size {hist_size} --thread 1 2 3 4 5 6 7 8 9 10 11 12\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_hyper.csv\"], columns=hist_columns+[\"cycles\"])\n",
    "#hist_data[\"ET\"] = hist_data[\"ET\"]/hist_data[\"thread\"].apply(lambda x: 1 if x == 0 else x)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"thread\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_cache_misses\"]*hist_data[\"thread\"]\n",
    "display(hist_data[hist_columns])\n",
    "plotPE(df=hist_data, lines=True, what=[(\"thread\", \"speedup\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "For thread counts 7-12 some of the threads are sharing a core.  As you can see, there's a hit to performance when we go to 7 cores.  This because the execution time of the whole program is the execution time of the _slowest_ thread.  In this case, that is either thread 1 or thread 7, since they share a core.\n",
    "\n",
    "Adding more hyperthreads improves performance but not as much as adding more full processors.  For instance, doubling the number of logical processors from 6 to 12 only yield only about 1.3x speedup, while going from 3 to 6 gave us 2x.\n",
    "\n",
    "Still, 1.3x is pretty good, really, since it didn't require any additional hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "And here's merge sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"merge_sort.cpp\", function=\"merge_sort\", analyze=False, opt=\"-Og\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size {8*1024*1024} --arg1 {8*1024*1024} {4*1024*1024} {2*1024*1024} {1024*1024} {512*1024}\" , \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df = render_csv(\"build/merge_sort.csv\")\n",
    "df[\"i\"] = df.index\n",
    "df[\"threshold\"] = df[\"arg1\"]\n",
    "df[\"approx threads\"] = df[\"size\"]/df['threshold']\n",
    "df[\"speedup\"] = df.iloc[0][\"ET\"]/df[\"ET\"]\n",
    "display(df[[\"size\", \"threshold\", \"approx threads\", \"ET\", \"speedup\"]])\n",
    "plotPE(df=df, lines=True, what=[(\"approx threads\", \"speedup\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The story is pretty similar:  From 2 to 4 threads gives 1.6x, while going from 4 to 8 only gives us 1.24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Using locks and  threads to parallelize code is notoriously tricky.  I chose the histogram and merge sort examples intentionally because they are pretty simple.  Fortunately, the compiler can help us a great deal for certain kinds of code.\n",
    "\n",
    "Compilers have had good success automatically parallelizing code for well-behaved loops.  \"Well-behaved\" means loops with loop bounds that don't change and that increment their index variables by fixed amounts.  Many programs fall into this category, including matrix multiplication and our histogram code.  Merge sort, by contrast, does not.\n",
    "\n",
    "OpenMP (Open Multi-Processing) is a widely-used and widely-supported set of extensions for C/C++ (and Fortran, if you're into that) that let the programmer guide the compiler to parallelize loops.\n",
    "\n",
    "The extensions take the form of `#pragma`s.  OpenMP has many of them, but we will use three:\n",
    "\n",
    "* `#pragma omp parallel for`\n",
    "* `#pragma omp critical`\n",
    "* `#pragma omp simd`\n",
    "\n",
    "`omp parallel for` tells the compiler to parallelize the following loop.\n",
    "\n",
    "`omp critical` tells the compiler that the next block of code should be treated as a critical section.\n",
    "\n",
    "`omp simd` tell the compiler to try to vectorize the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Thread Parallelism in OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Let's take a look at threads first.  Here's the multi-threaded OpenMP version of our histogram code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=(\"//START_OPENMP\", \"//START_OPENMP\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "This code is roughly equivalent to the course-grain locking version we wrote earlier.  Internally, OpenMP divides the iterations of the outer loop into chunks, and sends them threads to execute them.  Pretty slick!  It create a single lock to protect the histogram update.  Probably not so slick.  \n",
    "\n",
    "Let's see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"OPENMP\"]=\"yes\"\n",
    "t = fiddle(\"histogram.cpp\", function=\"run_openmp_histogram\",  name=\"histogram_openmp\", analyze=False, opt=\"-O3\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size 10000000 --thread 1 2 3 4 5 6\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_openmp.csv\"], columns=hist_columns+[\"cycles\"])\n",
    "#hist_data[\"ET\"] = hist_data[\"ET\"]/hist_data[\"thread\"].apply(lambda x: 1 if x == 0 else x)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"thread\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_cache_misses\"]*hist_data[\"thread\"]\n",
    "display(hist_data)\n",
    "plotPE(df=hist_data, lines=True, what=[(\"thread\", \"speedup\"), ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "That's awful!  But not unexpected.  One lock means no concurrency and a shared histogram array means lots of sharing.  We should have known better!\n",
    "\n",
    "Here's a better OpenMP version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=(\"//START_OPENMP_PRIVATE\", \"//START_OPENMP_PRIVATE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Note that  we did the blocking ourselves this time.  OpenMP breaks the execution of the outer loop into tiles, but the local set of histogram counters, `my_histogram`, is local _to the loop iteration_ it is in.  If we didn't tile the outer loop ourselves, the lock in the `omp critical` would get taken/released on _every iteration_.  By tiling the loop ourselves, we collect a bunch of updates in `my_histogram` and then apply them all at once.   In this case, the tiling doesn't provide a locality benefit (because `data` is never reused), but it does reduce locking overhead!  \n",
    "\n",
    "In particular, by setting `arg1` to 1000, we should reduce locking overhead by about 1000 times.\n",
    "\n",
    "Let's see how this version does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"histogram.cpp\", function=\"run_openmp_private_histogram\",  name=\"histogram_openmp_private\", analyze=False, opt=\"-O3\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size {hist_size} --thread 1 2 3 4 5 6 --arg1 1000\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_openmp_private.csv\"], columns=hist_columns+[\"cycles\"])\n",
    "#hist_data[\"ET\"] = hist_data[\"ET\"]/hist_data[\"thread\"].apply(lambda x: 1 if x == 0 else x)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"thread\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_cache_misses\"]*hist_data[\"thread\"]\n",
    "display(hist_data[hist_columns])\n",
    "plotPE(df=hist_data, lines=True, what=[(\"thread\", \"speedup\"), ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Pretty good:  5.95x speedup with 6 threads (there's some variation from run to run.).  And the code is much simpler with OpenMP!  Sounds good to me.  \n",
    "\n",
    "Especially, if I had to do PA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## SIMD Parallelism in OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "OpenMP also support SIMD parallelism, which uses vector instructions to improve performance.  We discussed SIMD during the 142L lecture last week.  Review the podcast, if you missed it.\n",
    "\n",
    "Applying SIMD with OpenMP is pretty easy:  You just put `#pragma omp simd` before the loop body.  The loop needs to very regular (e.g., constant stride and no branches), and there's no guarantee that the compiler will be able to vectorize it.  Indeed, `omp simd` doesn't speedup our histogram code, so we'll look at a simpler example.\n",
    "\n",
    "Here's an example of using SIMD parallelism for dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"dp.cpp\", function=\"dp\", analyze=False, opt=\"-O3\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t vsum(uint64_t *a, uint64_t* b, uint64_t * c, uint64_t len)\n",
    "{\n",
    "    uint64_t s = 0;\n",
    "    for(unsigned int i = 0; i < len; i++) {\n",
    "        c[i]=a[i]+b[i];\n",
    "    }\n",
    "    return s;\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t vsum_simd(uint64_t *a, uint64_t* b, uint64_t * c, uint64_t len)\n",
    "{\n",
    "    uint64_t s = 0;\n",
    "#pragma omp simd\n",
    "    for(unsigned int i = 0; i < len; i++) {\n",
    "        c[i]=a[i]+b[i];\n",
    "    }\n",
    "    return s;\n",
    "}\n",
    "\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* dp(uint64_t threads, uint64_t * list, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    if(arg1 == 0) {\n",
    "        list[0] = vsum(list, &list[size/3], &list[size*2/3], size/3);\n",
    "    } else if(arg1 == 1){\n",
    "        list[0] = vsum_simd(list, &list[size/3], &list[size*2/3], size/3);\n",
    "    }\n",
    "\treturn list;\n",
    "}\n",
    "FUNCTION(one_array_2arg, dp);\n",
    "\n",
    "\n",
    "\"\"\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size 10000000 --thread 1 --arg1 0 1\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df = render_csv(\"build/dp.csv\")\n",
    "df[\"opt\"] = df[\"arg1\"].apply(lambda x: [\"no SIMD\", \"SIMD\"][x])\n",
    "df[[\"opt\", \"IC\", \"CPI\", \"CT\", \"ET\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "1.16x speedup for one line of code is pretty good!   Note IC dropped but CPI and CT went up a bit!\n",
    "\n",
    "Here's the assembly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "compare([do_cfg(\"build/dp.so\", symbol=\"vsum\"), do_cfg(\"build/dp.so\", symbol=\"vsum_simd\")], [\"No SIMD\", \"SIMD\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "It's the `dq` suffixes and the `xmm` registers in the big basic block on the right that show we are using vectors.\n",
    "\n",
    "This code is not taking full advantage of our machine, however.  By default, gcc only uses the mmx instructions that can use 128-bit vectors.  Our processor has AVX, which supports 256-bit.  We can turn this on with `-march=broadwell`, since `broadwell` is the name of the specific version of the Intel architecture in our processors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!rm -f build/dp.o build/dp.so\n",
    "t = fiddle(\"dp.cpp\", function=\"dp\", analyze=False, opt=\"-O3 -march=broadwell\", run=[\"perf_count\"], \n",
    "           cmdline=f\"--size 10000000 --thread 1 --arg1 0 1\", \n",
    "           perf_cmdline=\"--stat-set L1.cfg --MHz 3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df = render_csv(\"build/dp.csv\")\n",
    "df[\"opt\"] = df[\"arg1\"].apply(lambda x: [\"no SIMD\", \"SIMD\"][x])\n",
    "df[[\"opt\", \"IC\", \"CPI\", \"CT\", \"ET\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Speedup is the same, but IC dropped significantly while CPI went up dramatically.\n",
    "\n",
    "Here's the assembly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "compare([do_cfg(\"build/dp.so\", symbol=\"vsum\"), do_cfg(\"build/dp.so\", symbol=\"vsum_simd\")], [\"No SIMD\", \"SIMD\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Now it's using `ymm` registers, with are 256 bits.\n",
    "\n",
    "All-in-all, SIMD is a mixed bag on our machine but it doesn't seem to hurt much and can improve performance a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Programming Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Your programming assignment in this lab is an extension of your work on the last lab.  This time, you'll be parallelizing your implementation of matrix exponentiation.  You can use C++ threads, pthreads, or OpenMP.  The lab assume you'll use OpenMP.  If you use one of the others, the course staff will  be less able to help you out.\n",
    "\n",
    "You are free to use your (and only your) solution to the previous lab as a starting point for this lab.\n",
    "\n",
    "Most of this section is a verbatim copy of the PA for Lab 4.  The sections with new content have \"(New for Lab 5)\" at the end of their names.\n",
    "\n",
    "Obviously, the performance targets have been increased.  Roughly speaking, the targets have increased by about 5.5x.  There are six cores on our machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142L.is_response": true,
    "cse142L.points": 3,
    "cse142L.question_type": "correctness",
    "deletable": false,
    "editable": true
   },
   "source": [
    "<div class=\"question correctness points-3\">\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "\n",
    "If you must achieve a total speedup of 5.5x using 6 processors, what fraction of the program must you parallelize (assuming perfect 6-thread parallelism on those parts)? \n",
    "\n",
    "</div>\n",
    "    \n",
    "<div class= \"answer\">\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Performance Variability (New for Lab 5)\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Multithreaded Performance is Variable**:  Multithreading introduces variability in performance.  You will only get credit for performance numbers recorded on gradescope, you may need to run your gradescope job more than once to get meaningful measurements.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Running code on multiple processors introduce performance variability -- some times up to 20% or more.  In the real world, you'd either tolerate this or spend a lot of time trying to fix it.  Neither of those works well in a lab, because we have limited time and you need a grade that reflects the quality of your work rather than whether a run was lucky or unlucky.\n",
    "\n",
    "To address this problem, the lab includes does two things:\n",
    "1.  It runs your code 4 times and takes the average.  This is implemented in the `Makefile`.\n",
    "2.  It includes a \"canary\" program with known performance.  \n",
    "\n",
    "The canary runs before your code and if the performance of the canary is too low, the autograder will reject the run.  This means that it is much more likely that your gradescope submissions will fail and you may need to submit several times to get a good measurement. \n",
    "\n",
    "With the canary filtering out slow runs, performance variation is less this 5%.  So there is some marginal value in submitting multiple times in the hopes of getting a slightly good score.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Budget time for multiple submissions**:  Having to resubmit to gradscope due to canary failure is totally predictable, and you have been warned.  Plan to submit (and resubmit as necessary) well-ahead of the deadline. \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "## Reference Code (New For Lab 5)\n",
    "\n",
    "The reference implementation is in `matexp_reference.hpp`.  It's basically the same as for Lab 4, but it demonstrates `wall_time()` (see below) and it makes the tensor copy explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"matexp_reference.hpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Read through the code and comments to make sure you understand what the code is doing. \n",
    "\n",
    "## Detailed Requirements\n",
    "\n",
    "The requirements for the lab are pretty simple:\n",
    "\n",
    "1. $M$ will be square and it's width/height will be less than 2048.\n",
    "2. $p$ will be less than or equal to 1024.\n",
    "3. $p$ will be greater than or equal to 0.\n",
    "4. Like `matexp_reference`, your function need to be a template function, but you can assume that `T` is always `uint64_t`.\n",
    "5. Values in $M$ can be any `uint64_t` value.\n",
    "6. Your output must match the output of the code in `matexp_reference.hpp`.\n",
    "7. Your implementation should go in `matexp_solution.hpp`.  The starter version is just a copy of `matexp_reference.hpp`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Running the Code\n",
    "\n",
    "The driver code for the lab is in `matexp_main.cpp` and `matexp.cpp`.  `matexp_main.cpp` is mostly command line processing (take a look if you want).  `matexp.cpp` is what actually calls your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"matexp.cpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "It defines four functions:\n",
    "\n",
    "* `matexp_reference_c` Calls the starter code with `size`x`size` matrix and `power`.\n",
    "* `matexp_solution_c` Calls your code with `size`x`size` matrix and `power`.\n",
    "* `bench_reference` Runs benchmarks we will use for grading for the starter code.\n",
    "* `bench_solution` Runs benchmarks we will use for grading for your code.\n",
    "\n",
    "It runs everything 8 times to get a more reliable measurement.\n",
    "\n",
    "To invoke these, you can build and run `matexp.exe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make matexp.exe\n",
    "!./matexp.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "`matexp.exe` takes several command line parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!./matexp.exe --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The notable ones are:\n",
    "\n",
    "1. `--size` -- set the size of the matrix to multiply.\n",
    "2. `--power` -- set the power to raise it to.\n",
    "3. `--thread` -- set the number of threads to use.\n",
    "3. `--p1` to `--p5` -- set parameters (see below.)\n",
    "4. `--function` what functions to run.\n",
    "5. `--seed` set the random seed.\n",
    "6. `--stats-file` sets where statistics should go.\n",
    "\n",
    "The first six of these can take multiple values and `matexp.exe` will run all combinations and they will end up in `stats.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!./matexp.exe --function matexp_reference_c matexp_solution_c --size 10 20 --power 4 8\n",
    "render_csv(\"stats.csv\", columns=[\"function\", \"size\", \"power\", \"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "And, of course, we run it all in the cloud (we've added `--stat-set L1.cfg` to gather cache and TLB statistics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make matexp.exe\n",
    "!cse142 job run --lab parallel \"./matexp.exe --stat-set L1.cfg --function matexp_reference_c matexp_solution_c --size 10 20 --power 4 8\"\n",
    "render_csv(\"stats.csv\", columns=pa_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Setting Parameters\n",
    "\n",
    "One of the key parts of this lab is setting parameters (e.g., tiling sizes), and the `matexp.exe` has support for this built in vias the `--p1` -- `--p5` command line options and function parameters.\n",
    "\n",
    "You can use these for whatever you'd like:\n",
    "\n",
    "1.  Setting tile sizes.\n",
    "2.  Selecting among different implementations.\n",
    "3.  Whatever else.\n",
    "\n",
    "Their default value is 1.\n",
    "\n",
    "Just like `--size`, `--power`, and `--thread`, you can multiple values and `matexp.exe` will run all combinations.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!cse142 job run --lab parallel \"./matexp.exe --stat-set L1.cfg --function matexp_reference_c matexp_solution_c --thread 1 6 --size 10 20 --power 4 8 --p1 1 2 --p2 3 4 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_csv(\"stats.csv\", columns=pa_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Tips for Using Parameters**\n",
    "\n",
    "1.  Running multiple values of multiple parameters can result in a lot of experiments... sometimes too many.\n",
    "2.  Jobs in the cloud are limited to 5 minutes, so you need to limit the number of tests per job.\n",
    "3.  That said, exploring a wide space of parameter settings can be an effective way to optimize your code.  There are tips in the \"Looking at Data Section\" about how to deal with lots of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## The Test Suite\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Tests are great:** Tests are about the best thing ever (although writing them is a hassle). If you run the tests consistently, you can worry _much_ less about correctness.  Make small incremental changes to your code, run the tests after each change and enjoy the warm glow of happiness when they pass!\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**NOTE:** You normally will not need to run `run_tests.exe` in the cloud.  It'll work fine, but it takes longer which will slow your work down.  The test suite is about _correctness_ not performance.\n",
    "     \n",
    "</div>\n",
    "\n",
    "The lab  provides a comprehensive test suite for your implementation.  The code in is `run_test.cpp`.  `run_tests.exe` also takes the `--p*` arguments so you can run the regressions with different parameter settings.  This is a _good_ idea.\n",
    "\n",
    "For this lab, the autograder runs the tests with multiple threads.\n",
    "\n",
    "You can build the tests with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make run_tests.exe\n",
    "!./run_tests.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Test Suite Details\n",
    "\n",
    "You can list all the tests with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./run_tests.exe --gtest_list_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The first group of tests is (under `MatexpTests`), contains four simple tests that call `do_simple_diag_test()` and `do_simple_offdiag_test()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"run_tests.cpp\", show=(\"//START1\", \"//END1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "These two functions take a diagonal or off-diagonal matrix and raise them to a power.  It's easy to calculate the correct results for these computations, so they make good tests.\n",
    "\n",
    "These tests are:\n",
    "\n",
    "1. `one_test` runs the two functions above for a given size.\n",
    "2. `simple_tests` runs the two functions above for a set of small test cases.\n",
    "3. `simple_random_tests` runs the two functions for 10 randomly generate test cases.\n",
    "\n",
    "The final test in this group (`randomize_tests`) calls `do_test()` which compares the output `matexp_reference.hpp` with `matexp_solution.hpp` for randomized test cases.\n",
    "\n",
    "The second group of tests the `size`s and `power`s that the benchmark uses.\n",
    "\n",
    "The third group of tests (under `MatexpTests/MatexpTestFixture`) calls `do_test` with a bunch of test cases of various sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## OpenMP (New For Lab 5)\n",
    "\n",
    "OpenMP is automatically turned for your code in this lab, so the `#pragma` command we used for the histogram should work fine.\n",
    "\n",
    "If you want to compile at the command line, you'll need to either do \n",
    "\n",
    "```\n",
    "export OPENMP=yes\n",
    "```\n",
    "\n",
    "once each time you log in or invoke `make` like so:\n",
    "\n",
    "```\n",
    "make matexp.exe OPENMP=yes\n",
    "```\n",
    "each time you build.\n",
    "\n",
    "### Key Commands\n",
    "\n",
    "The three `#pragma`s you'll need for this lab are \n",
    "\n",
    "1. `#pragma omp parallel for` for parallelizing loops.\n",
    "2. `#pragma omp critical` for parallelizing loops.\n",
    "3. `#pragma omp simd` for vectorizing loops\n",
    "\n",
    "These are the only three used in the solution used to set the performance targets.\n",
    "\n",
    "These three blog posts provide a good introduction to these commands:\n",
    "\n",
    "* http://jakascorner.com/blog/2016/04/omp-introduction.html  \n",
    "* http://jakascorner.com/blog/2016/05/omp-for.html\n",
    "* http://jakascorner.com/blog/2016/07/omp-critical.html\n",
    "\n",
    "They are required reading.\n",
    "\n",
    "These articles provide some more advanced topics that might be useful:\n",
    "\n",
    "* http://jakascorner.com/blog/2016/06/omp-for-scheduling.html\n",
    "* http://jakascorner.com/blog/2016/06/omp-data-sharing-attributes.html\n",
    "* http://jakascorner.com/blog/2016/07/omp-default-none-and-const.html\n",
    "\n",
    "There's an enormous amount of bad information online about OpenMP.\n",
    "\n",
    "### Looking at OpenMP Assembly\n",
    "\n",
    "If you look at the assembly for OpenMP programs, you'll find that your loop body has been replaced with a function call.  OpenMP does this so it can tell it's worker threads to call the function to perform one iteration of your loop.\n",
    "\n",
    "### Caveats for our Tools\n",
    "\n",
    "First, `gprof` doesn't work on with multi-threaded programs.  You can use it for single-threaded runs, though.\n",
    "\n",
    "Second, Moneta's cache model is not multithread-aware, so the cache hit/miss numbers for multithreaded programs are not meaningful.\n",
    "\n",
    "Moneta may also show more threads that you might be expecting.  OpenMP threads seem to be Thread 0 and 13 and above.  If you see some threads that don't seem to be doing anything, that's not surprising or concerning.\n",
    "\n",
    "Finally, our performance counting code only collects data for one thread.  For OpenMP code this is ok:  all the threads do basically the same thing.  But you'll notice, for instance, that if a loop runs in 4 threads, the measure instruction count will go down by ~1/4 (assuming multi-threading didn't add a lot of overhead).\n",
    " \n",
    " \n",
    "### Controlling the Number of Threads\n",
    "\n",
    "By default the autograder will run your code with 12 threads.  If you want to use a different number in your final run, you can call \n",
    "\n",
    "```\t\t\t\n",
    "omp_set_num_threads(thread_count);\n",
    "```\n",
    "\n",
    "in your function. You should call it before the first OpenMP `#pragma`.\n",
    "\n",
    "I can control thread count during development with `--thread`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Things To Try\n",
    "\n",
    "There are two main challenges I see in this lab:\n",
    "\n",
    "1. Make matrix multiplication fast, primarily by improving it's memory behavior.\n",
    "2. Applying matrix multiplication efficiently to compute $M^p$.\n",
    "\n",
    "The benchmarks are structured to evaluate your solution's success on both of these challenges.\n",
    "\n",
    "### Tiling Matrix Multiplication\n",
    "\n",
    "The obvious approach to improving cache performance is tiling and renesting.  You saw an example of this with 1-D convolution, and the principle is the same, but the problem is a little more complex because there is an extra loop.\n",
    "\n",
    "There are two ways to approach this task and you should try to apply both at once:\n",
    "\n",
    "1.  You should think about the data access pattern of matrix multiply in terms of temporal and spatial locality.  \n",
    "    1.  How can you maximize spatial locality?\n",
    "    2.  Don't forget to consider all three matrices.\n",
    "    3.  How large can the tile size be while still fitting in the cache?\n",
    "2.  You should try different tiling schemes:\n",
    "    1.  Different ways to split and renest the three loops.\n",
    "    2.  Different tile sizes (`--p1` to `--p5` are provided for this purpose)\n",
    "3.  Debugging tiling\n",
    "    1.  Debugging tiling can be tricky.\n",
    "    2.  Start with small matrices and small tile sizes.\n",
    "    3.  Try multiple small tile sizes (pass `--p*` to the regressions)\n",
    "3.  Don't forget about loop overhead.\n",
    "4.  Tiling is effective, but not as effective as it was for 1-D convolution.\n",
    "\n",
    "There's a nice [Wikipedia page](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm) about matrix multiplication.  It covers the theory behind implementing it effectively.  The content is good, but don't assume that theory and practice will match.\n",
    "\n",
    "\n",
    "### Raising to a Power\n",
    "\n",
    "Computing $M^p$ can done more efficiently than multiplying $M$ by itself $p$ times (which is what the reference code does).  By way of a hint, remember that:\n",
    "\n",
    "$$M^{p+q} = M^pM^q$$\n",
    "\n",
    "As you work on this part of the problem, I suggest practicing with integers first.  I found it useful to code my solution with integers and test it and then rewrite it for matrices.\n",
    "\n",
    "It's possible to compute $M^p$ in $O(\\log p)$ multiplications.\n",
    "\n",
    "\n",
    "\n",
    "### Using the Test Suite\n",
    "\n",
    "The test suite is meant to help keep you on the right track as you go through the assignment.  When you make a change to you code, I would:\n",
    "\n",
    "1.  Run all the tests.  If they all pass, great!\n",
    "2.  If some fail, run `simple_tests` and then `simple_random_tests`.\n",
    "3.  Once I find a particular test case that fails, I'd use `one_test` to run just that configuration while debugging.\n",
    "\n",
    "If and when you make use of parameters (`--p1` etc.), I'd try out the values of interest with `./run_tests.exe` before bothering to run code in the cloud.\n",
    "\n",
    "One debugging tip:  `tensor_t.hpp` includes support for the `<<` operator so you can say\n",
    "\n",
    "```\n",
    "std::cerr << my_tensor\n",
    "```\n",
    "\n",
    "This can be very helpful when debugging.\n",
    "\n",
    "### Non-Deterministic Tests (New for Lab 5)\n",
    "\n",
    "With threading, comes non-deterministic bugs.  This means that the tests may fail only occasionally for your code.  If this seems to be happening, a good strategy is to just run them repeatedly and confirm that it's the case.\n",
    "\n",
    "It's not a bug in the test suite, you have a thread synchronization error.\n",
    "\n",
    "\n",
    "### General Tips (New for Lab 5)\n",
    "\n",
    "In the examples we saw that loop tiling and OpenMP pragmas can work well together.  This carries through to how you should figure out what to parallelize.  It's worth your time to try parallelizing different loops and changing how your loops are nested to accommodate that.\n",
    "\n",
    "A few tips:\n",
    "\n",
    "1.  Use `wall_time()` (below) guide your optimizations.\n",
    "1.  At this points you have many tools available to you -- `omp parallel for`, `omp simd`, compiler optimizations, tiling.  The number of combinations is enormous.   I suggest applying them in this order (from largest impact-per-effort to smallest):\n",
    "    1.  Get last lab into good shape (see notes above and slides from class)\n",
    "    2.  `omp parallel for`\n",
    "    3.  `omp simd`\n",
    "    4.  Fiddling with other compiler options/per-function compiler options.\n",
    "    5.  Crazy stuff like intrinsics for better SIMD performance.\n",
    "1.  While tiling only applied to loops with reuse (because temporal locality requires reuse), `parallel for` can apply to loops without reuse.  Same for `omp simd`.\n",
    "2.  `omp parallel for` implicitly does tiling, since it divides the iterations of the parallel loop across several cores.\n",
    "1.  Nesting parallel for loops with OpenMP is not usually a good idea (although it should work).  Start by picking one loop to parallelize.\n",
    "2.  You want to parallelize an outer loop, so that the threads are working on large pieces of computation and need to synchronize less.\n",
    "3.  Pay close attention to whether iterations of your parallel loop are writing to the same locations.  If so, you'll need a `omp critical` to ensure correct updates.\n",
    "\n",
    "This last point can be tricky.  If I have this code:\n",
    "\n",
    "```\n",
    "#pragma omp parallel for\n",
    "for(int i = 0; i < 10; i++) {\n",
    "    for(int j = 0; j < 10; j++) {\n",
    "        for(int k = 0; k < 10; k++) {\n",
    "            X(i,j) += Y(k,j);\n",
    "```\n",
    "\n",
    "The only store is the assignment to `X(i, j)`.  Since `i` is the index of the parallel loop, I know that no other thread will be updating `X(i,j)`, since no other thread will have the same value of `i`.\n",
    "\n",
    "However, in this code:\n",
    "\n",
    "```\n",
    "#pragma omp parallel for\n",
    "for(int i = 0; i < 10; i++) {\n",
    "    for(int j = 0; j < 10; j++) {\n",
    "        for(int k = 0; k < 10; k++) {\n",
    "            X(k,j) += Y(i,j);\n",
    "```\n",
    "\n",
    "I don't have the same guarantee.  Since `i` does is not used to select an element in `X`, every other thread will write to that location as well.  In that case, I could do\n",
    "\n",
    "```\n",
    "#pragma omp parallel for\n",
    "for(int i = 0; i < 10; i++) {\n",
    "    for(int j = 0; j < 10; j++) {\n",
    "        for(int k = 0; k < 10; k++) {\n",
    "#pragma omp critical\n",
    "            X(k,j) += Y(i,j);\n",
    "```\n",
    "Which will probably be really slow, or I could create a private tensor, do my updates there, and then merge them into `X`.\n",
    "\n",
    "5.  Pay close attention to write sharing when you are deciding how to parallelize.  Can the iterations of your parallel loop iterations write to the same place?\n",
    "4.  `omp simd` only works on inner loops.\n",
    "5.  `omp parallel for` works best on outer loops.\n",
    "7.  `gprof` doesn't work for multithreading, so use `wall_time()` to measure how long things take.\n",
    "\n",
    "Regarding #7:  If you've parallelize part of your program, and speedup is good but not great, use `wall_time()` to identify serial parts of your code that are slow.   The problem may be that your $x$ in Amdahl's Law is too small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Useful C++ ( Partly New For Lab 5) \n",
    "\n",
    "#### `wall_time()` (New for Lab 5)\n",
    "\n",
    "We've provide `wall_time()` which returns the current time as a `double`.  It has microsecond accuracy.  You can call it before and after part of your program, subtract the resulting values, and get a pretty good measure of execution time.  There's an example in `matexp_reference.hpp`.\n",
    "\n",
    "#### Controlling Compiler Optimizations\n",
    "\n",
    "First, you can prevent inlining of a particular function by declaring it like so:\n",
    "\n",
    "```\n",
    "void __attribute__((noinline)) matexp_solution(...)\n",
    "```\n",
    "\n",
    "This can make it easier to debug, because you can set a breakpoint on the function and it'll work like you expect.\n",
    "\n",
    "Second, you can turn on arbitrary optimizations for particular functions like so:\n",
    "\n",
    "```\n",
    "#pragma GCC push_options\n",
    "#pragma GCC optimize (\"unroll-loops\")\n",
    "\n",
    "void your_function() {\n",
    "}\n",
    "\n",
    "#pragma GCC pop_options\n",
    "```\n",
    "\n",
    "\n",
    "#### Assertions\n",
    "\n",
    "The `assert()` macro is useful tool for debugging and to avoid silly errors.\n",
    "\n",
    "If you say\n",
    "\n",
    "```\n",
    "assert(a > b);\n",
    "```\n",
    "\n",
    "And the expression is not true at run time, the assert with \"fail\" your program will crash with a somewhat useful error message.\n",
    "\n",
    "This is a useful way to document and enforce assumptions you make in your code.  For instance, I used an assert in `convolution_tiled_split()` to ensure that the tile size was > 8.\n",
    "\n",
    "You can get access to  `assert()` with \n",
    "\n",
    "```\n",
    "#include<cassert>\n",
    "```\n",
    "\n",
    "The overhead of asserts is low, but not zero.  I would not put any in one of your performance-critical loops.\n",
    "\n",
    "If you want to include asserts in performance-critical areas, you can add `-DNDEBUG` to the optimizations in `config.make`.  It'll disable all the `assert()`s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Do Your Work Here\n",
    "\n",
    "Below are the key commands you'll need to make progress on the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Setting Optimization Flags\n",
    "\n",
    "As in your last lab, you can set optimization flags in `config.make`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "render_code(\"config.make\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Compiling and Running\n",
    "\n",
    "You can compile and the benchmarks locally using this command.  This is only useful for debugging.  Performance running locally is not very meaningful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make matexp.exe\n",
    "!./matexp.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Run the benchmark in the cloud and compare your performance with the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make clean\n",
    "!make matexp.exe\n",
    "!cse142 job run --lab parallel --take ./matexp.exe --take L1.cfg  --force \"./matexp.exe --MHz 3500 --stat-set ./L1.cfg  --thread 1 6 12 --function bench_solution  --p1 1 --p2 1 --p3 1  --p4 1 --p5 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "render_csv(\"stats.csv\", columns=pa_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "This command will approximate what the autograder will do, but it let's you pass `--p*` parameters.  The cells below will show your results and what the autograder will do with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make matexp.exe\n",
    "!cse142 job run --lab parallel --force  \"./matexp.exe --MHz 3500 --stats bench1.csv --stat-set ./L1.cfg --canary 500000000 --function bench_solution --thread 12  --p1 1 --p2 1 --p3 1 --p4 1 --p5 1\"\n",
    "!./summarize.py --out bench.csv bench1.csv \n",
    "render_csv(\"bench.csv\", columns=pa_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "from autograde import compute_all_scores\n",
    "df = compute_all_scores(dir=\".\")\n",
    "display(df)\n",
    "print(f\"total points: {round(sum(df['score']), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Running Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Build the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make run_tests.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Here's a good order run the tests in.\n",
    "\n",
    "**Run one test**\n",
    "\n",
    "This is most useful for debugging.  Running a single test doesn't tell you much about your code's correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./run_tests.exe --gtest_filter=MatexpTests.one_test --size 10 --power 20 --thread 1 6 --p1 1 --p2 1 --p3 1 --p4 1 --p5 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Run the simple tests**\n",
    "\n",
    "Do this for checking basic correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./run_tests.exe --gtest_filter=MatexpTests.simple* --thread 1 6  --p1 1 --p2 1 --p3 1 --p4 1 --p5 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Run more regressions (everything except the benchmark regressions with take a long time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./run_tests.exe --gtest_filter=-MatexpBench*  --thread 1 6  --p1 1 --p2 1 --p3 1 --p4 1 --p5 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Run all the tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./run_tests.exe  --thread 1 6  --p1 1 --p2 1 --p3 1 --p4 1 --p5 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Looking at Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The `--p*` command line options and parameters give you the ability to test many different configurations for your algorithms.  You should use them!  \n",
    "\n",
    "The result will be lots of data in lots of csv files.  For instance, if you run the command in the previous section, you'll get `stats.csv`.  Let's generate another stats file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!cse142 job run --lab parallel --force \"./matexp.exe --MHz 3500 --thread 1 6 --stats other_stats.csv --stat-set ./L1.cfg  --function bench_solution\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "  You can load and view several at once like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df = render_csv([\"stats.csv\", \"other_stats.csv\"], columns=pa_columns).sort_values(by=\"ET\").head(2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "That command collects all the `.csv` files provide, selects a useful set of columns for this PA, sorts by `ET`, and display the top 2 elements.\n",
    "\n",
    "If you'd rather work on the data in Excel (or whatever), you can export it as a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"my_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Keeping Track of Your Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "During this lab, you'll try lots of things, and it can be hard to keep track of what code produced what results.  To make this easier, I've provide a little script called `run_and_snap.sh`.  Here's what it does:\n",
    "\n",
    "1. It creates a new directory called `snapshot_`$n$ where $n$ is unique number.\n",
    "2. It copies `matexp_solution.hpp` and `config.make` into that directory.\n",
    "3. It runs \n",
    "\n",
    "```\n",
    "make matexp.exe\n",
    "cse142 job run --lab parallel --force \"make autograde\";\n",
    "./autograde.py --submission . --results results.json --scores scores.csv\n",
    "```\n",
    "\n",
    "4. And displays the results.\n",
    "\n",
    "You can try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./run_and_snap.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "And look at the results for that run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_csv(\"scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "So it will save the code and results for your run.  \n",
    "\n",
    "Then, you can list the 10 snapshots that perform best on each of the three workloads.  If you want to see more increase 10 in `[:10]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df=render_csv(\"snapshot_*/scores.csv\", add_file=True)\n",
    "display(df[df[\"label\"]==\"bench_solution-120 320\"].sort_values(\"ET\")[:10])\n",
    "display(df[df[\"label\"]==\"bench_solution-350 25\"].sort_values(\"ET\")[:10])\n",
    "display(df[df[\"label\"]==\"bench_solution-600 2\"].sort_values(\"ET\")[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Snapshots are not submissions**:  We will not _under any circumstances_ accept snapshot results as a substitute for submitting via gradescope.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Tools\n",
    "\n",
    "These are some tools you might find useful as you optimize your implementation.  I encourage you to give some of them a try. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Debugging Regressions \n",
    "\n",
    "If a regression fails, `run_tests.exe` will tell you which test failed.  Here are some tips for debugging.  First, get a list of the tests:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "One of them will be the test that failed.  Then you can debug in gdb (at a terminal again):\n",
    "\n",
    "```\n",
    "bash$ gdb run_tests.exe\n",
    "(gdb) run --gtest_filter=<name_of_failing_test> --gtest_break_on_failure\n",
    "```\n",
    "The `--gtest_filter` just runs one test.  and `--gtest_break_on_failure` will stop drop you into the debugger if the error occurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Looking At Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "As you learned in the previous lab, name mangling makes it a little tricky to inspect the details of what the compiler does to C++ code, especially when it uses templates.  So let's see how we can track down the assembly for for your implementation.\n",
    "\n",
    "The `Makefile` is set up to build assembly files (ending in `.s`) in the `build` directory.  All the assembly for `matexp_solution.hpp` and `matexp_reference.hpp` (and a whole bunch of other stuff) will be in `matexp.s`.   It's quite long, so searching through it by hand is daunting.  To make matters worse, all the function names are mangled.\n",
    "\n",
    "One solution to this is to `c++filt` to demangle the names and the use `grep` to find the symbols of interest (the `^` matches the beginning of the line`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make build/matexp.s\n",
    "!c++filt < build/matexp.s | grep '^void matexp_solution<unsigned long>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You may see multiple functions listed with suffixes like `[clone .constprop.163]`.  These are specialized versions of the function that gcc produced.  They have had constant propagation applied to them for particular sets of values.  Initially, at least, I'd pay most attention to the generic version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You can see that there are several different versions of each method, one for each set of template parameters.  Unless you're doing something very sophisticated with your implementations, the assembly will all be basically the same.\n",
    "\n",
    "You can now render the assembly here with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make build/matexp.s\n",
    "render_code(\"build/matexp.s\", show=\"void matexp_solution<unsigned long>(tensor_t<unsigned long>&, tensor_t<unsigned long> const&, unsigned int, long, long, long, long, long)\", lang=\"gas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Looking at the CFG\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**CFG Viewer Problems:**  In testing the lab, I've found that the CFG viewer is having trouble displaying some of the functions.  The symptom is that it runs forever.  If that happens, take a look at the assembly instead.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "Control flow graphs are easier to interpret than the assembly, but getting them for C++ functions is also a little complicated.  The tool that our CFG generator is built on uses its own name mangling scheme internally.  To get the names it uses for your functions you can use the command below.  We pass the executable to `cfg` along with `--filter` which takes a string to search for.  If you leave out `--filter` you will all 2890 symbols in the executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make matexp.exe\n",
    "!cfg matexp.exe --filter mult_reference --list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "There's a one-to-one correspondence between these names the names we saw earlier.  You can render a CFG like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "do_cfg(\"matexp.exe\", symbol=\"sym.void_mult_reference_unsigned_long__tensor_t_unsigned_long___tensor_t_unsigned_long__const__tensor_t_unsigned_long__const_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "It's probably a bit of a mess.  I suggest copying it, downloading it, or opening it in it's own tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Profiling \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Gprof doesn't work on multi-threaded code.**  You can use `--thread 1` to run your code while profiling or use `walltime()` (described above) to measure where your program is spending time when running multi-threaded.\n",
    "\n",
    "</div>\n",
    "\n",
    "Profiling can be valuable tool in figuring out where your code is spending time.  \n",
    "\n",
    "To profile your allocator, you need to recompile it with profiling enabled:\n",
    "\n",
    "**NOTE:** Don't forget to rebuild without `GPROF=yes`. BUilding in support for gprof will slow down your code a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make clean matexp.exe GPROF=yes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You'll need to profile one type of benchmark at a time.  just run one of the lines below at a time.  For good accuracy, you should profile in the cloud.  \n",
    "\n",
    "Think carefully about to profile:\n",
    "\n",
    "1.  It's not a bad idea to profile the whole benchark, but it can be a bit hard to interpret the results, because there's a lot going on.\n",
    "2.  The results are sometimes clearer if you focus on just one test case.\n",
    "3.  Make sure you run a large enough test so that `matexp.exe` spends the vast majority of its time in your code.  This can be surprisingly large: `--size 600` is a good place start.\n",
    "\n",
    "Here's how to profile in the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!cse142 job run --lab parallel --take matexp.exe --force \"./matexp.exe --MHz 3500 --function bench_reference; gprof -l ./matexp.exe > gprof.out\"\n",
    "!cat gprof.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The output is a big for Jupyter Notebook.  In a terminal you can do:\n",
    "\n",
    "```\n",
    "less -S gprof.out\n",
    "\n",
    "```\n",
    "\n",
    "Which will let you look at the file without wrapped lines.\n",
    "\n",
    "The `-l` option to gprof gives you per-line profiling, so the entries in the file are lines rather than functions.\n",
    "\n",
    "You may notice some unfamiliar functions.  Here's what some of them are:\n",
    "\n",
    "1.  `nlohmann::basic*`: This is a json library.  If you spend much time here, you aren't running a large enough test case.\n",
    "2.  `boost::*` or `OptionSpec` various utilities for command line parsing and parameter passing.  Again, if you see this, run a larger test case.\n",
    "3.  `mcount` is the gprof instrumentation function.  It gets called a lot, but it's not there.\n",
    "\n",
    "Another problem you may run into:  the compiler may inline everything so all the time is one function.  This is not very informative.  You can get around by turning off inlining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!make clean matexp.exe GPROF=yes OPTIMIZE=\"-O3 -fno-inline\"\n",
    "!cse142 job run --lab parallel --force \"./matexp.exe --MHz 3500 --function matexp_solution_c --size 350 --power 10; gprof -l ./matexp.exe > gprof.out\" # Run one test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!cat gprof.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "You might notice that performance drops significantly!  The resulting profile has more detail, but it's also not as accurate a reflection of your real program.  However, it can provide useful guidance about where you code is spending time.  YOu have to be careful though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Debugging \n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Debug Single-threaded Code** Debugging multi-threaded code is possible but it's more complicated than single-threaded code.  For this lab, most bugs just need single-thread debugging.  If you have bugs that _only_ appear with multiple threads, make sure you are using `omp critical` correctly.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Your code will certainly have errors in it, and you'll need to debug.  THe first thing you need to do is to tone down the optimizations, because they make debugging almost impossible.  Recall that `-Og` is the right flag to use for optimization while debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make clean matexp.exe OPTIMIZE=\"-Og\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Unfortunately, the Linux debugger `gdb` doesn't work inside the note book.  If you want to use it, you can do so at the terminal:\n",
    "\n",
    "```\n",
    "$ gdb matexp.exe\n",
    "GNU gdb (Ubuntu 8.1.1-0ubuntu1) 8.1.1\n",
    "Copyright (C) 2018 Free Software Foundation, Inc.\n",
    "License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n",
    "This is free software: you are free to change and redistribute it.\n",
    "There is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\n",
    "and \"show warranty\" for details.\n",
    "This GDB was configured as \"x86_64-linux-gnu\".\n",
    "Type \"show configuration\" for configuration details.\n",
    "For bug reporting instructions, please see:\n",
    "<http://www.gnu.org/software/gdb/bugs/>.\n",
    "Find the GDB manual and other documentation resources online at:\n",
    "<http://www.gnu.org/software/gdb/documentation/>.\n",
    "For help, type \"help\".\n",
    "Type \"apropos word\" to search for commands related to \"word\"...\n",
    "Reading symbols from matexp.exe...rdone.\n",
    "(gdb) run --function matexp_solution_c --size 30 --power 3\n",
    "Starting program: /cse142L/labs/CSE141pp-Lab-Caches-II/matexp.exe --function matexp_solution_c --size 30 --power 3\n",
    "warning: Error disabling address space randomization: Operation not permitted\n",
    "[Thread debugging using libthread_db enabled]\n",
    "Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n",
    "registering function: bench_solution\n",
    "registering function: bench_reference\n",
    "registering function: matexp_reference_c\n",
    "registering function: matexp_solution_c\n",
    "Loading Native engine.\n",
    "Gonna run matexp_solution_c\n",
    "Running matexp_solution_c\n",
    ".\n",
    "[Inferior 1 (process 61156) exited normally]\n",
    "(gdb)\n",
    "```\n",
    "\n",
    "The best place to start is at `matexp_solution_c`.  From there you can step into your solution code.\n",
    "\n",
    "```\n",
    "bash$ gdb alloc_main.exe\n",
    "(gdb) break matexp_solution_c\n",
    "(gdb) run --function matexp_solution_c --size 100  --power 2\n",
    "(gdb) list\n",
    "```\n",
    "\n",
    "Sometimes that will note give a good result, even without optimizations.  Instead, you can set a break point at a line number:\n",
    "\n",
    "```\n",
    "bash$ gdb alloc_main.exe\n",
    "(gdb) break matexp_solution.hpp:47\n",
    "(gdb) run --function matexp_solution_c --size 100  --power 2\n",
    "(gdb) list\n",
    "```\n",
    "\n",
    "There's a pretty good `gdb` [tutorial here](https://www.cs.cmu.edu/~gilpin/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Tracing With Moneta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Moneta was originally built specifically to help with visualizing cache tiling optimizations.  Here's a few tips for getting good results:\n",
    "\n",
    "1.  Datahub can only handle traces with about 10 million memory operations, so keep the test cases small. `--size 60` is good.\n",
    "2.  Use the code in `matexp_reference.hpp` as a guide for how to tag the matrices.\n",
    "3.  Click on \"Accesses layers\" in the trace viewer to see which tags are which.\n",
    "4.  Click on \"None\" next to the gold square and select \"misses-all\" to see where the misses are.\n",
    "5.  Click on \"tags\" and then click on the check boxes to show and hide tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make clean matexp.exe \n",
    "!mtrace --memops 10000000 -- ./matexp.exe --size 60 --power 2 --function matexp_reference_c --p1 1 --p2 1 --p3 1 --p5 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "show_trace(\"./trace_0\",  show_tag=[\"dst\", \"B\", \"A\"], layer_preset=[\"None\", \"dst\", \"B\", \"A\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Final Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "When you are done, make sure your best allocator is called `matexp_solution()` in `matexp_solution.hpp`.  Your code will be invoked with `--p1` to `--p5` equal to 1, so you'll need to \"bake in\" the optimal values for those parameters.\n",
    "\n",
    "Then you can submit your code to the Gradescope autograder.  It will run the commands given above and use the `ET` values from `autograde.csv` to assign your grade.\n",
    "\n",
    "Your grade is based on your speed up relative `matexp_reference.hpp` on three benchmarks. \n",
    "\n",
    "For each of them, there's a target speedup given in the table.  You get a score for each benchmark between 0 and 33.3, and the overall score is the sum of these scores.  For each function, the score is compute as `your_speedup/target_speedup * 33.3`.\n",
    "\n",
    "For this lab, you don't get extra credit for beating the targets.  This will help ensure that your design in balanced:  You much do well at all 3 benchmarks to do well on the lab.\n",
    "\n",
    "To get points, your code must also be correct.  The autograder will run the regressions in `run_tests.exe` to check it's correctness.\n",
    "\n",
    "You can mimic exactly what the autograder will do with the command below.  You can run the cell below to list them and the target speedups.\n",
    "\n",
    "After you run it, the results will be in `autograde/autograde.csv` rather than `./bench.csv`.  This command builds and runs your code in a more controlled way by doing the following:\n",
    "\n",
    "1.  Ignores all the files in your repo except `matexp_solution.hpp` and `config.make`.\n",
    "2.  Copies those files into a clean clone of the starter repo.\n",
    "3.  Builds `matexp.exe` from scratch.\n",
    "4.  And then runs the commands the benchmarks.\n",
    "5.  It then runs the `autograde.py` script to compute your grade.\n",
    "\n",
    "Running the cell does just what the Gradescope autograder does.  And the cell below shows the name and target speedups for each benchmark.  This takes 1-2 minutes to run.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Only Gradescope Counts** The scores produced here **do not** count.  Only gradescope counts.  The results here should match what Gradescope does, but I would test your solution on Gradescope well-ahead of the deadline to ensure your code is working like you expect.\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**The autograder doesn't pass parameters**  The autograder will not pass any `-p*` parameters to your code.   You'll need to set it up so the default value (1 for all the `--p*` arguments) configures your code in the best way possible.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!make matexp.exe\n",
    "!cse142 job run --take matexp_solution.hpp --take config.make --lab parallel-bench --force  autograde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_csv(\"autograde/bench.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "And run the autograder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!./autograde.py --submission autograde --results autograde.json\n",
    "from autograde import compute_all_scores\n",
    "df = compute_all_scores(dir=\"autograde\")\n",
    "display(df)\n",
    "print(f\"total points: {round(sum(df['score']), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The \"capped_score\" column contains the number of points you'll receive.\n",
    "\n",
    "And see the autograder's output like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "render_code(\"autograde.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Most of it is internal stuff that gradscope needs, but the key parts are the `score`, `max_score`, and `output` fields.\n",
    "\n",
    "All that's left is commit your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!git commit -am \"Solution to the lab.\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "If `git push` asks for your username, you'll need to push from the command line.\n",
    "\n",
    "If `git commit` tells you have uncommitted files, that's not a problem. \n",
    "\n",
    "If `git commit` tell you something like:\n",
    "\n",
    "```\n",
    "*** Please tell me who you are.\n",
    "\n",
    "Run\n",
    "\n",
    "git config --global user.email \"you@example.com\"\n",
    "git config --global user.name \"Your Name\"\n",
    "\n",
    "to set your account's default identity.\n",
    "Omit --global to set the identity only in this repository.\n",
    "\n",
    "fatal: unable to auto-detect email address (got 'prcheng@dsmlp-jupyter-prcheng.(none)')\n",
    "Warning: Permanently added the RSA host key for IP address '140.82.112.3' to the list of known hosts.\n",
    "Everything up-to-date\n",
    "```\n",
    "\n",
    "Then you can do (but fill in your @ucsd.edu email and your name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cse142L.is_response": true,
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!git config --global user.email \"you@example.com\"\n",
    "!git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Modern computers rely on multiple forms of parallelism to achieve good performance.  Instruction-level, memory-level, vector, and thread parallelism all have a role to play.  Unfortunately (for programmers), modern processors have reached the limits of how much ILP and MLP can be extracted and exploited from typical programs.  For further performance gains, we need to turn to more temperamental forms of parallelism like vectors and thread.  Threads provide the most flexible form of parallelism and, given the ever-growing number of cores in modern systems, the most scalable path to high performance.  However, extracting thread-level parallelism adds overhead and creates new challenges in optimizing memory usage -- sharing (both false and real) can quickly destroy threads' potential benefits.  What's more, Amdahl's law can limit the multi-thread speedup possible for most algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Turning In the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "For each lab, there are two different assignments on gradescope:\n",
    "\n",
    "1.  The lab notebook.\n",
    "2.  The programming assignment.\n",
    "\n",
    "There's also a pre-lab reading quiz on Canvas and a post-lab survey which is embedded below.\n",
    "\n",
    "## If You Have Trouble\n",
    "\n",
    "If it's near the deadline and you are having trouble turning in any part of your lab, you can fill out this form: https://forms.gle/ThHjESfbZRqqztXUA to let us know what's going on and provide us access to the work you have done prior to the deadline.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**NOTE:** Filling out the form above _before_ the deadline is the _only_ mechanism available to receive credit without turning in the assignment on time.\n",
    "    \n",
    "</div>\n",
    "\n",
    "If it's more than a day before the deadline, you can reach out via Piazza and hopefully we can get it sorted out.\n",
    "\n",
    "\n",
    "## Reading Quiz\n",
    "\n",
    "The reading quiz is an online assignment on Canvas.  It's due before the class when we will assign the lab.\n",
    "\n",
    "## The Note Book\n",
    "\n",
    "You need to turn in your lab notebook and your programming assignment separately. \n",
    "\n",
    "After you complete the lab, you will turn it in by creating a version of the notebook that only contains your answers and then printing that to a pdf.\n",
    "\n",
    "**Step 1:**  Save your workbook!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!for i in 1 2 3 4 5; do echo Save your notebook!; sleep 1; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Step 2:**  Run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!turnin-lab Lab.ipynb\n",
    "!ls -lh Lab.turnin.ipynb"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAACUAAAAgCAYAAACVU7GwAAABQ2lDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rACYQiDNwMeonJxQWOAQE+QCUMMBoVfLvGwAiiL+uCzBKZ0nzEVv7xlbCa77O3fegRxFSPArhSUouTgfQfIE5LLigqYWBgTAGylctLCkDsDiBbpAjoKCB7DoidDmFvALGTIOwjYDUhQc5A9g0gWyA5IxFoBuMLIFsnCUk8HYkNtRcEeFxcfXwUAkyMDc0DCTiXdFCSWlECop3zCyqLMtMzShQcgaGUquCZl6yno2BkYGTIwAAKc4jqn2/BYclYtxkhlhjMwGDYChQUQohlizIw7PnNwCC0GyGmlcfAINjAwLA/viCxKBHuAMZvLMVpxkYQtngYAwNn1///L4Ae5f4HtEvv//8fvP///65nYGAvYmDotgIAsv9dz+zV67cAAAA4ZVhJZk1NACoAAAAIAAGHaQAEAAAAAQAAABoAAAAAAAKgAgAEAAAAAQAAACWgAwAEAAAAAQAAACAAAAAAe+CsRQAAAshJREFUWAntl89LKlEUx786kotQaaMh2KYnprixQlBeq/ZBgUtduvI/E1y0aRkuJDcp1ErQVSsNlELSovE9v9d3ffOjGWcg0UUHhnPv3PPj4znXe9XzZy7YMvFuGY/A2Uoon7ZS3V4P7VYL7+/v2tdrHfv9fmSOj/Hr8HCZRwfVur9HNBqFoijCwOPxLA2/eyC3sqqqohCWUB8fH/D5fCCM17vo7DrAJNBsNhO5jJ3RVYrVYJUIRBj5rKNKBGMeghnFEsrrJdz3gxGGz2zGRxUffCUU6dlCam3FpONoNEK/30csFsPr6yteXl7kkqWORCLY29sT6/+hZnMoDz4/P01+pkotKuT910ZlrvWnxs3NDWq1GiqVCur1Oh4fH01BjS+urq5QKpWWr1WVe0kVc+YzigmKEIsqKfOKKcvyyg0fj8eRz+dxcHCATCaDQCBgjGma04d7lVWiMBYLxLmimC8UE5R0Ipzc6BKIa+FwGIlEAqFQSLRQHh9csxL6yFgSjPHZvq/EBCWdpbFx3mw2Ua1WEQwGcXt7i4eHB2lqqQuFAo6OjpbrEswYWxqYoOQCNZ2MksvlRMtSqZTQp6enRhPTPJ1O6959FVdrYAulNZTjweAZnU4HyWQST09P6Ha7cslSh8MR0XJLA8OCa6her4tGowFWqN1uO2ofr66zs9+G1NZT11AXFxfIZrPi28cW8qxaJfv7+6tMdOuuoXg2XV9fo1wu4+7uztE5xQ9yeXmpS2w3cQ01Ho8xGAzw9vaG4XAoxnYJuEYfN+Iaqlgsgg/l/PzcTS7Htvo7xLHbeg1/oJzW17ZS8jpwGsyp3aq4po1OB62Tduw0qZ2dNp4xl/TTQe3s7GA6nWJ3dxeL3zyLu4/Oq+4rGdBOSyBqxqeeTCbgPxqt6KCOT0429hdLC+WZ05p/ZWktNjC23egb4BEpf6CcVn4rK/UXyU7+WMGqw1YAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The date in the above file listing should show that you just created `Lab.turnin.ipynb`\n",
    "\n",
    "**Step 3:**  Click on this link to open it: [./Lab.turnin.ipynb](./Lab.turnin.ipynb)\n",
    "\n",
    "**Step 4:**  Hide the table of contents by clicking the\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "**Step 5:**  Select \"Print\" from _your browser's_ \"file\" menu.  Print directly to a PDF.\n",
    "\n",
    "**Step 6:**  Make sure all your answers are visible and not cut off the side of the page.\n",
    "\n",
    "**Step 7:**  Turn in that PDF via gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Print Carefully** It's important that you print directly to a PDF.  In particular, you should _not_ do any of the following:\n",
    "    \n",
    "1. **Do not** select \"Print Preview\" and then print that. (Remarkably, this is not the same as printing directly, so it's not clear what it is a preview of)\n",
    "2. **Do not** select `Download as-> PDF via LaTex.  It generates nothing useful.\n",
    "    \n",
    "</div>\n",
    "\n",
    "In gradescope, you'll need to show us where all your answers are.  Please do this carefully, if we can't find your answer, we can't grade it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## The Programming Assignment\n",
    "\n",
    "You'll turn in your programming assignment by providing gradescope with your github repo.   It'll run the autograder and return the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Lab Survey\n",
    "\n",
    "Please fill out this survey when you've finished the lab.  You can only submit once.  Be sure to press \"submit\", your answers won't be saved in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame    \n",
    "IFrame('https://docs.google.com/forms/d/1xIYT16NuMV_ocYWY3ttDuoR01yt4if63mOviZRyAUXg/viewform?embedded=true', width=800, height=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Ignore this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "335px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
